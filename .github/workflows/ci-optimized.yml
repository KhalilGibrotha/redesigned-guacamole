---
name: ğŸš€ CI/CD Pipeline

on:
  workflow_call:
    inputs:
      full_scan:
        description: 'Run full codebase scan (not just changed files)'
        required: false
        type: boolean
        default: true
      branch_name:
        description: 'Branch name to checkout'
        required: false
        type: string
        default: ''
    secrets:
      CONFLUENCE_URL:
        required: false
      CONFLUENCE_USER:
        required: false
      CONFLUENCE_API_TOKEN:
        required: false

jobs:
  # Job to detect what types of files have changed for optimized execution
  detect-changes:
    name: ğŸ” Detect File Changes
    runs-on: ubuntu-latest
    outputs:
      docs-changed: ${{ steps.changes.outputs.docs }}
      ansible-changed: ${{ steps.changes.outputs.ansible }}
      python-changed: ${{ steps.changes.outputs.python }}
      workflows-changed: ${{ steps.changes.outputs.workflows }}
      any-code-changed: ${{ steps.changes.outputs.code }}
      external-call: ${{ steps.context.outputs.external-call }}
    steps:
      - name: ğŸ“¥ Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: ğŸ” Check Repository Context
        id: context
        run: |
          echo "Current repository: ${{ github.repository }}"
          echo "Expected repository: KhalilGibrotha/redesigned-guacamole"
          
          if [[ "${{ github.repository }}" != "KhalilGibrotha/redesigned-guacamole" ]]; then
            echo "external-call=true" >> $GITHUB_OUTPUT
            echo "ğŸ”„ External repository call detected - will assume docs changed"
          else
            echo "external-call=false" >> $GITHUB_OUTPUT
            echo "ğŸ  Internal repository call - will detect actual changes"
          fi

      - name: ğŸ” Detect Changed Files
        uses: dorny/paths-filter@v3
        id: changes
        with:
          filters: |
            docs:
              - 'docs/**'
              - '*.md'
              - 'README*'
            ansible:
              - 'playbooks/**'
              - 'roles/**'
              - 'inventory/**'
              - 'site.yml'
              - 'playbook*.yml'
              - 'playbook*.yaml'
              - '.ansible-lint'
              - 'ansible.cfg'
            python:
              - '**/*.py'
              - 'requirements*.txt'
              - 'pyproject.toml'
              - '.flake8'
              - '.pylintrc'
            workflows:
              - '.github/workflows/**'
              - '.github/super-linter*'
            code:
              - '**/*.py'
              - '**/*.yml'
              - '**/*.yaml'
              - '**/*.sh'
              - '**/*.json'
              - '.github/workflows/**'

  super-linter:
    name: ğŸ” Super Linter
    runs-on: ubuntu-latest
    needs: detect-changes
    if: needs.detect-changes.outputs.any-code-changed == 'true' || inputs.full_scan == true

    permissions:
      contents: read
      packages: read
      statuses: write
      security-events: write

    steps:
      - name: ğŸ“¥ Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: ğŸ”§ Configure Environment
        id: config
        run: |
          # Dynamic branch detection
          if [ "${{ github.event.repository.default_branch }}" != "" ]; then
            echo "default_branch=${{ github.event.repository.default_branch }}" >> $GITHUB_OUTPUT
          else
            echo "default_branch=main" >> $GITHUB_OUTPUT
          fi

      - name: ğŸ” Verify Linter Configuration Files
        run: |
          echo "ğŸ” Checking for linter configuration files..."

          # Check for .ansible-lint
          if [ -f ".ansible-lint" ]; then
            echo "âœ… .ansible-lint found ($(wc -l < .ansible-lint) lines)"
            echo "ğŸ“„ .ansible-lint preview:"
            head -5 .ansible-lint
          else
            echo "âŒ .ansible-lint not found"
          fi

          # Check for .yamllint
          if [ -f ".yamllint" ]; then
            echo "âœ… .yamllint found ($(wc -l < .yamllint) lines)"
          else
            echo "âŒ .yamllint not found"
          fi

          # Check for markdownlint config (prefer .json)
          if [ -f ".markdownlint.json" ]; then
            echo "âœ… .markdownlint.json found ($(wc -l < .markdownlint.json) lines)"
          elif [ -f ".markdownlint.yml" ]; then
            echo "âœ… .markdownlint.yml found ($(wc -l < .markdownlint.yml) lines)"
          else
            echo "âŒ No markdownlint config found"
          fi

          # Check for any conflicting config files
          if [ -f ".markdownlint.json" ] && [ -f ".markdownlint.yml" ]; then
            echo "âš ï¸  Both .markdownlint.json and .markdownlint.yml found - Super Linter will prefer .json"
          fi

          echo "ğŸ¯ Super Linter will auto-detect all configuration files at repo root"

      - name: ğŸ” Detect Files for Validation
        id: detect-files
        run: |
          echo "ğŸ” Detecting file types for intelligent validation..."
          
          # Check for different file types and set outputs (with proper quoting)
          yaml_files=$(find . -type f \( -name "*.yml" -o -name "*.yaml" \) ! -path "./.git/*" | wc -l)
          ansible_files=$(find . -type f \( -path "./playbooks/*" -o -path "./roles/*" -o -name "site.yml" -o -name "playbook*.yml" -o -name "playbook*.yaml" \) | wc -l)
          python_files=$(find . -type f -name "*.py" ! -path "./.git/*" ! -path "./.venv/*" ! -path "./venv/*" | wc -l)
          markdown_files=$(find . -type f -name "*.md" ! -path "./.git/*" | wc -l)
          shell_files=$(find . -type f -name "*.sh" ! -path "./.git/*" | wc -l)
          json_files=$(find . -type f -name "*.json" ! -path "./.git/*" ! -path "./node_modules/*" | wc -l)
          github_actions_files=$(find .github/workflows -type f \( -name "*.yml" -o -name "*.yaml" \) 2>/dev/null | wc -l)
          
          # Set outputs
          echo "yaml_files=$yaml_files" >> $GITHUB_OUTPUT
          echo "ansible_files=$ansible_files" >> $GITHUB_OUTPUT
          echo "python_files=$python_files" >> $GITHUB_OUTPUT
          echo "markdown_files=$markdown_files" >> $GITHUB_OUTPUT
          echo "shell_files=$shell_files" >> $GITHUB_OUTPUT
          echo "json_files=$json_files" >> $GITHUB_OUTPUT
          echo "github_actions_files=$github_actions_files" >> $GITHUB_OUTPUT
          
          # Output summary
          echo "ğŸ“Š File detection summary:"
          echo "  - YAML files: $yaml_files"
          echo "  - Ansible files: $ansible_files"
          echo "  - Python files: $python_files"
          echo "  - Markdown files: $markdown_files"
          echo "  - Shell files: $shell_files"
          echo "  - JSON files: $json_files"
          echo "  - GitHub Actions files: $github_actions_files"

      - name: âš™ï¸ Generate Dynamic Super Linter Configuration
        id: generate-config
        run: |
          echo "âš™ï¸ Generating dynamic Super Linter configuration..."
          
          # Set boolean values based on file detection
          yaml_validation=$([ "${{ steps.detect-files.outputs.yaml_files }}" -gt 0 ] && echo "true" || echo "false")
          ansible_validation=$([ "${{ steps.detect-files.outputs.ansible_files }}" -gt 0 ] && echo "true" || echo "false")
          markdown_validation=$([ "${{ steps.detect-files.outputs.markdown_files }}" -gt 0 ] && echo "true" || echo "false")
          python_validation=$([ "${{ steps.detect-files.outputs.python_files }}" -gt 0 ] && echo "true" || echo "false")
          shell_validation=$([ "${{ steps.detect-files.outputs.shell_files }}" -gt 0 ] && echo "true" || echo "false")
          json_validation=$([ "${{ steps.detect-files.outputs.json_files }}" -gt 0 ] && echo "true" || echo "false")
          github_actions_validation=$([ "${{ steps.detect-files.outputs.github_actions_files }}" -gt 0 ] && echo "true" || echo "false")
          
          # Set validate all codebase based on input or event type
          if [[ "${{ inputs.full_scan }}" == "true" ]] || [[ "${{ github.event_name }}" == "schedule" ]] || [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            validate_all_codebase="true"
          else
            validate_all_codebase="false"
          fi
          
          # Create the dynamic configuration file
          cat > .github/super-linter-dynamic.env << 'ENVEOF'
          # Super Linter Dynamic Configuration
          # Generated by CI workflow based on runtime conditions
          
          ##########################
          # Branch and Git Settings
          ##########################
          
          # Default branch for comparison (dynamically set)
          DEFAULT_BRANCH=BRANCH_PLACEHOLDER
          
          # Validate all files vs changed files only
          VALIDATE_ALL_CODEBASE=VALIDATE_ALL_PLACEHOLDER
          
          ##########################
          # Intelligent Validation Toggles
          # (Based on file presence detection)
          ##########################
          
          # Enable validation only if files of that type exist
          VALIDATE_YAML=YAML_PLACEHOLDER
          VALIDATE_ANSIBLE=ANSIBLE_PLACEHOLDER
          VALIDATE_MARKDOWN=MARKDOWN_PLACEHOLDER
          VALIDATE_PYTHON=PYTHON_PLACEHOLDER
          VALIDATE_BASH=BASH_PLACEHOLDER
          VALIDATE_JSON=JSON_PLACEHOLDER
          VALIDATE_SHELL_SHFMT=SHELL_PLACEHOLDER
          VALIDATE_GITHUB_ACTIONS=GITHUB_ACTIONS_PLACEHOLDER
          
          ##########################
          # Auto-fix Configuration
          ##########################
          
          # Enable auto-fixing for supported linters (only if files exist)
          FIX_YAML=FIX_YAML_PLACEHOLDER
          FIX_MARKDOWN=FIX_MARKDOWN_PLACEHOLDER
          FIX_JSON=FIX_JSON_PLACEHOLDER
          FIX_SHELL_SHFMT=FIX_SHELL_PLACEHOLDER
          
          ##########################
          # Performance Settings
          ##########################
          
          # Multi-threading for faster execution
          PARALLEL=true
          SLIM_IMAGE=true
          FAIL_FAST=false
          
          ##########################
          # Output Configuration
          ##########################
          
          # Detailed output for CI analysis
          OUTPUT_FORMAT=tap
          OUTPUT_DETAILS=detailed
          CREATE_LOG_FILE=true
          
          ##########################
          # File Filtering
          ##########################
          
          # Filter settings
          USE_FIND_ALGORITHM=false
          IGNORE_GITIGNORED_FILES=true
          
          ##########################
          # Suppression
          ##########################
          
          # Suppress certain outputs for cleaner logs
          SUPPRESS_POSSUM=true
          SUPPRESS_FILE_TYPE_WARN=true
          ENVEOF
          
          # Replace placeholders with actual values
          sed -i "s/BRANCH_PLACEHOLDER/${{ steps.config.outputs.default_branch }}/g" .github/super-linter-dynamic.env
          sed -i "s/VALIDATE_ALL_PLACEHOLDER/${validate_all_codebase}/g" .github/super-linter-dynamic.env
          sed -i "s/YAML_PLACEHOLDER/${yaml_validation}/g" .github/super-linter-dynamic.env
          sed -i "s/ANSIBLE_PLACEHOLDER/${ansible_validation}/g" .github/super-linter-dynamic.env
          sed -i "s/MARKDOWN_PLACEHOLDER/${markdown_validation}/g" .github/super-linter-dynamic.env
          sed -i "s/PYTHON_PLACEHOLDER/${python_validation}/g" .github/super-linter-dynamic.env
          sed -i "s/BASH_PLACEHOLDER/${shell_validation}/g" .github/super-linter-dynamic.env
          sed -i "s/JSON_PLACEHOLDER/${json_validation}/g" .github/super-linter-dynamic.env
          sed -i "s/SHELL_PLACEHOLDER/${shell_validation}/g" .github/super-linter-dynamic.env
          sed -i "s/GITHUB_ACTIONS_PLACEHOLDER/${github_actions_validation}/g" .github/super-linter-dynamic.env
          sed -i "s/FIX_YAML_PLACEHOLDER/${yaml_validation}/g" .github/super-linter-dynamic.env
          sed -i "s/FIX_MARKDOWN_PLACEHOLDER/${markdown_validation}/g" .github/super-linter-dynamic.env
          sed -i "s/FIX_JSON_PLACEHOLDER/${json_validation}/g" .github/super-linter-dynamic.env
          sed -i "s/FIX_SHELL_PLACEHOLDER/${shell_validation}/g" .github/super-linter-dynamic.env
          
          echo "âœ… Dynamic configuration generated with intelligent validation"
          echo "ğŸ“‹ Configuration preview:"
          head -20 .github/super-linter-dynamic.env
          echo ""
          echo "ğŸ¯ Validation summary:"
          echo "  - YAML validation: ${yaml_validation} (files: ${{ steps.detect-files.outputs.yaml_files }})"
          echo "  - Ansible validation: ${ansible_validation} (files: ${{ steps.detect-files.outputs.ansible_files }})"
          echo "  - Markdown validation: ${markdown_validation} (files: ${{ steps.detect-files.outputs.markdown_files }})"
          echo "  - Python validation: ${python_validation} (files: ${{ steps.detect-files.outputs.python_files }})"
          echo "  - Shell validation: ${shell_validation} (files: ${{ steps.detect-files.outputs.shell_files }})"

      - name: ğŸ” Run Super Linter
        id: super-linter
        uses: super-linter/super-linter@v5
        env:
          # Use dynamically generated configuration
          ENV_FILE: .github/super-linter-dynamic.env
          
          # GitHub token (required for API access)
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: ğŸ“Š Analyze Auto-fixes Applied
        if: always()
        id: autofix-analysis
        run: |
          echo "ğŸ” Analyzing auto-fixes applied by Super Linter..."

          # Initialize counters
          total_fixes=0
          yaml_fixes=0
          ansible_fixes=0
          python_fixes=0
          shell_fixes=0
          markdown_fixes=0
          json_fixes=0

          # Function to count fixes in git diff
          count_file_changes() {
            local file_pattern="$1"
            local category="$2"

            if git diff --name-only | grep -E "$file_pattern" > /dev/null 2>&1; then
              local changed_files=$(git diff --name-only | grep -E "$file_pattern" | wc -l)
              local total_changes=$(git diff --numstat | grep -E "$file_pattern" | awk '{sum += $1 + $2} END {print sum+0}')

              echo "ğŸ“ $category fixes: $changed_files files, $total_changes changes"
              return $total_changes
            else
              echo "ğŸ“ $category fixes: 0 files, 0 changes"
              return 0
            fi
          }

          # Check if there are any changes from auto-fix
          if git diff --quiet; then
            echo "âœ… No auto-fixes were needed - code already compliant!"
            echo "autofix_needed=false" >> $GITHUB_OUTPUT
            echo "total_fixes=0" >> $GITHUB_OUTPUT
          else
            echo "ğŸ”§ Auto-fixes were applied!"
            echo "autofix_needed=true" >> $GITHUB_OUTPUT

            # Count fixes by category
            count_file_changes "\\.ya?ml$" "YAML"; yaml_fixes=$?
            count_file_changes "\\.py$" "Python"; python_fixes=$?
            count_file_changes "\\.sh$" "Shell/Bash"; shell_fixes=$?
            count_file_changes "\\.md$" "Markdown"; markdown_fixes=$?
            count_file_changes "\\.json$" "JSON"; json_fixes=$?

            # Special handling for Ansible (subset of YAML)
            if git diff --name-only | grep -E "(playbook|tasks|handlers|vars).*\\.ya?ml$|site\\.ya?ml$" > /dev/null 2>&1; then
              ansible_fixes=$(git diff --numstat | grep -E "(playbook|tasks|handlers|vars).*\\.ya?ml$|site\\.ya?ml$" | awk '{sum += $1 + $2} END {print sum+0}')
              echo "ğŸ“ Ansible fixes: $ansible_fixes changes"
            fi

            total_fixes=$((yaml_fixes + python_fixes + shell_fixes + markdown_fixes + json_fixes))

            # Output for next steps
            echo "total_fixes=$total_fixes" >> $GITHUB_OUTPUT
            echo "yaml_fixes=$yaml_fixes" >> $GITHUB_OUTPUT
            echo "ansible_fixes=$ansible_fixes" >> $GITHUB_OUTPUT
            echo "python_fixes=$python_fixes" >> $GITHUB_OUTPUT
            echo "shell_fixes=$shell_fixes" >> $GITHUB_OUTPUT
            echo "markdown_fixes=$markdown_fixes" >> $GITHUB_OUTPUT
            echo "json_fixes=$json_fixes" >> $GITHUB_OUTPUT

            # Show detailed diff summary
            echo ""
            echo "ğŸ“‹ Detailed changes by file:"
            git diff --name-status | while read status file; do
              if [ "$status" = "M" ]; then
                changes=$(git diff --numstat "$file" | awk '{print $1 + $2}')
                echo "  ğŸ“„ $file: $changes changes"
              fi
            done
          fi

      - name: ğŸ’¾ Commit Auto-fixes
        if: steps.autofix-analysis.outputs.autofix_needed == 'true'
        run: |
          echo "ğŸ’¾ Committing auto-fixes..."

          # Configure git (use GitHub Actions bot)
          git config --local user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"

          # Add all changes
          git add .

          # Create detailed commit message
          commit_msg="ğŸ¤– Auto-fix: Applied ${{ steps.autofix-analysis.outputs.total_fixes }} linting fixes

          Auto-fixes applied by Super Linter:
          - YAML fixes: ${{ steps.autofix-analysis.outputs.yaml_fixes }}
          - Ansible fixes: ${{ steps.autofix-analysis.outputs.ansible_fixes }}
          - Python fixes: ${{ steps.autofix-analysis.outputs.python_fixes }}
          - Shell fixes: ${{ steps.autofix-analysis.outputs.shell_fixes }}
          - Markdown fixes: ${{ steps.autofix-analysis.outputs.markdown_fixes }}
          - JSON fixes: ${{ steps.autofix-analysis.outputs.json_fixes }}

          Automated by: ${{ github.workflow }} #${{ github.run_number }}
          Triggered by: ${{ github.event_name }} on ${{ github.ref_name }}"

          # Commit changes
          git commit -m "$commit_msg"

          # Push changes back to the branch
          git push origin ${{ github.ref_name }}

          echo "âœ… Auto-fixes committed and pushed!"

      - name: ğŸ“¤ Upload Super Linter Logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: super-linter-logs-${{ github.run_id }}
          path: |
            super-linter.log
            github-super-linter.log
            .github/super-linter-dynamic.env
          retention-days: 30
          if-no-files-found: ignore

      - name: ğŸ“¤ Upload Dynamic Configuration (Debug)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: super-linter-config-${{ github.run_id }}
          path: .github/super-linter-dynamic.env
          retention-days: 7
          if-no-files-found: warn

  # Enhanced security scanning - only runs when code changes
  security:
    name: ğŸ›¡ï¸ Security Scan
    runs-on: ubuntu-latest
    needs: detect-changes
    if: needs.detect-changes.outputs.any-code-changed == 'true' || inputs.full_scan == true
    permissions:
      actions: read
      contents: read
      security-events: write
    steps:
      - name: ğŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ğŸ” Run DevSkim Scanner
        uses: microsoft/DevSkim-Action@v1

      - name: ğŸ“¤ Upload DevSkim SARIF
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: devskim-results.sarif

      - name: â˜¢ï¸ Run Trivy Vulnerability Scanner
        uses: aquasecurity/trivy-action@0.32.0
        with:
          scan-type: 'fs'
          ignore-unfixed: true
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH'

      - name: ğŸ“¤ Upload Trivy Scan Results
        uses: actions/upload-artifact@v4
        with:
          name: trivy-results
          path: trivy-results.sarif

      - name: ğŸ” Advanced Secret Detection
        run: |
          echo "ğŸ” Running security validation..."

          # Check for potential secrets (excluding false positives)
          echo "Checking for potential hardcoded secrets..."
          if grep -rE "(password|secret|api_key|auth_token|private_key):\s*['\"]?[A-Za-z0-9+/=]{10,}" . \
             --include="*.yml" --include="*.yaml" --include="*.py" --include="*.sh" \
             --exclude-dir=.git --exclude-dir=.github \
             --exclude="*example*" --exclude="*template*" \
             | grep -v "YOUR_.*_HERE\|test:test\|example\|template\|#.*token\|#.*secret\|README"; then
            echo "âš ï¸ Potential secrets found - please review"
            exit 1
          else
            echo "âœ… No obvious secrets detected"
          fi

      - name: ğŸ”’ File Permissions Check
        run: |
          echo "ğŸ”’ Checking file permissions..."

          # Check for world-writable files
          if find . -name "*.yml" -o -name "*.yaml" -o -name "*.py" -o -name "*.sh" | xargs ls -la | grep "^-.......rw"; then
            echo "âŒ World-writable files found"
            find . -name "*.yml" -o -name "*.yaml" -o -name "*.py" -o -name "*.sh" | xargs ls -la | grep "^-.......rw"
            exit 1
          else
            echo "âœ… File permissions look secure"
          fi

      - name: ğŸ›¡ï¸ Security Summary
        if: always()
        run: |
          echo "## ğŸ›¡ï¸ Security Validation Summary" >> $GITHUB_STEP_SUMMARY
          echo "âœ… **DevSkim scan completed**" >> $GITHUB_STEP_SUMMARY
          echo "âœ… **Secret detection completed**" >> $GITHUB_STEP_SUMMARY
          echo "âœ… **File permissions checked**" >> $GITHUB_STEP_SUMMARY

  # Publish job - runs on main/release/hotfix branches when docs change, regardless of linting results
  publish:
    name: ğŸš€ Publish to Confluence
    needs: [detect-changes]
    if: >
      github.event_name == 'push' &&
      (github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/heads/release/') || startsWith(github.ref, 'refs/heads/hotfix/')) &&
      (needs.detect-changes.outputs.docs-changed == 'true' || inputs.full_scan == true || needs.detect-changes.outputs.external-call == 'true')

    # Using the workflow from our repository to ensure script availability
    uses: KhalilGibrotha/redesigned-guacamole/.github/workflows/publish-docs.yml@main
    with:
      target_environment: 'production'
      dry_run: false

    # Pass the required secrets to the reusable workflow.
    secrets:
      CONFLUENCE_URL: ${{ secrets.CONFLUENCE_URL }}
      CONFLUENCE_USER: ${{ secrets.CONFLUENCE_USER }}
      CONFLUENCE_API_TOKEN: ${{ secrets.CONFLUENCE_API_TOKEN }}

  # Comprehensive report - always runs to provide summary
  comprehensive-report:
    name: ğŸ“Š Generate Comprehensive Report
    # This job runs after all checks are complete, including publish which now runs in parallel
    needs: [super-linter, security, detect-changes, publish]
    if: always() # Always run to report on success or failure
    runs-on: ubuntu-latest
    steps:
      - name: ï¿½ Checkout Code (for scripts)
        uses: actions/checkout@v4

      - name: ï¿½ğŸ“¥ Download all artifacts
        uses: actions/download-artifact@v4
        with:
          # Download all artifacts into a 'reports' directory
          path: ./reports
        continue-on-error: true

      - name: ğŸ Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'
          cache: 'pip'

      - name: ğŸ“¦ Install Dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi

      - name: ğŸ“ Generate Summary from All Reports
        run: |
          python3 -c "
          import json
          import os
          import glob
          import sys
          
          def parse_sarif(file_path):
              if not os.path.exists(file_path):
                  return '| N/A | No report found | This scan may have failed to produce a report. | N/A |\n', 0
          
              report_md = ''
              results = {}
              total_issues = 0
              
              try:
                  with open(file_path, 'r', encoding='utf-8') as f:
                      sarif_data = json.load(f)
                      for run in sarif_data.get('runs', []):
                          for result in run.get('results', []):
                              total_issues += 1
                              rule_id = result.get('ruleId', 'unknown')
                              message = result.get('message', {}).get('text', 'No description available')
                              level = result.get('level', 'warning').upper()
                              key = (level, rule_id, message)
                              results[key] = results.get(key, 0) + 1
              except (json.JSONDecodeError, KeyError, TypeError) as e:
                  return f'| ERROR | PARSE_ERROR | Failed to parse SARIF file: {str(e)} | N/A |\n', 0
              
              if not results:
                  report_md += '| âœ… | None | No issues found! | 0 |\n'
              else:
                  for (level, rule, msg), count in sorted(results.items()):
                      msg_escaped = msg.replace('|', '\\|').replace('\n', ' ').replace('\r', '').replace('`', '\\`')
                      report_md += f'| {level} | \`{rule}\` | {msg_escaped} | {count} |\n'
              
              return report_md, total_issues
          
          # Main execution
          summary_file = os.getenv('GITHUB_STEP_SUMMARY')
          if not summary_file:
              print('ERROR: GITHUB_STEP_SUMMARY environment variable not set', file=sys.stderr)
              sys.exit(1)
          
          reports_dir = './reports'
          overall_total = 0
          
          # Dynamically discover all SARIF reports
          all_reports = {}
          for p in glob.glob(os.path.join(reports_dir, '**', '*.sarif'), recursive=True):
              name = os.path.basename(os.path.dirname(p)).replace('-', ' ').replace('_', ' ').title()
              if name:
                  all_reports[name] = p
          
          print(f'Processing {len(all_reports)} report types...')
          print(f'Reports directory: {reports_dir}')
          print(f'Summary will be written to: {summary_file}')
          
          # Check if reports directory exists
          if not os.path.exists(reports_dir):
              print(f'WARNING: Reports directory {reports_dir} does not exist')
              print('This is normal if no artifacts were generated or downloaded.')
              with open(summary_file, 'a') as f:
                  f.write('## ğŸ“Š Security & Quality Reports Summary\n\n')
                  f.write('### âš ï¸ No Reports Found\n\n')
                  f.write('The reports directory was not found. This may indicate:\n')
                  f.write('- No artifacts were generated by the scanning jobs\n')
                  f.write('- All scans passed without issues (no SARIF files created)\n')
                  f.write('- Artifact download failed or was skipped\n\n')
                  f.write('**This is often normal** - it means no security or linting issues were detected!\n\n')
              print('âœ… Summary generated with no reports found message')
              sys.exit(0)
          
          # Check if directory is empty
          all_files = glob.glob(os.path.join(reports_dir, '**', '*'), recursive=True)
          report_files = [f for f in all_files if f.endswith('.sarif')]
          
          if not report_files:
              print(f'INFO: Reports directory exists but contains no SARIF files')
              print(f'Found {len(all_files)} total files in reports directory')
              with open(summary_file, 'a') as f:
                  f.write('## ğŸ“Š Security & Quality Reports Summary\n\n')
                  f.write('### âœ… No Issues Found\n\n')
                  f.write('Reports directory exists but no SARIF reports were found.\n')
                  f.write('This typically means all security and linting scans passed successfully!\n\n')
                  if len(all_files) > 0:
                      f.write(f'ğŸ“ **Files found in reports directory**: {len(all_files)}\n')
                      f.write('(These may be log files or other artifacts)\n\n')
              print('âœ… Summary generated with no issues found message')
              sys.exit(0)
          
          # Process each report
          with open(summary_file, 'a') as f:
              f.write('## ğŸ“Š Security & Quality Reports Summary\n\n')
          
          for tool_name, report_path in all_reports.items():
              print(f'Processing {tool_name} report: {report_path}')
              
              with open(summary_file, 'a') as f:
                  f.write(f'### {tool_name} Report\n')
                  f.write('| Severity | Rule ID | Description | Total |\n')
                  f.write('|----------|---------|-------------|-------|\n')
              
              report_content, issue_count = parse_sarif(report_path)
              overall_total += issue_count
              
              with open(summary_file, 'a') as f:
                  f.write(report_content)
                  f.write('\n')
              
              print(f'  - Found {issue_count} issues in {tool_name}')
          
          # Final summary
          with open(summary_file, 'a') as f:
              f.write('\n---\n\n')
              if overall_total == 0:
                  f.write('### ğŸ‰ Excellent! No Issues Found\n\n')
                  f.write('All security and quality scans completed successfully with no issues detected.\n')
              else:
                  f.write(f'### ğŸ“Š Total Issues Found: {overall_total}\n\n')
                  f.write('Please review the individual reports above for detailed information.\n')
          
          print(f'âœ… Summary generation complete. Total issues across all tools: {overall_total}')
          "

      - name: ğŸ“Š Execution Summary
        if: always()
        run: |
          echo "## ğŸš€ Workflow Execution Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ğŸ” Change Detection Results" >> $GITHUB_STEP_SUMMARY
          echo "- **Repository**: ${{ github.repository }}" >> $GITHUB_STEP_SUMMARY
          echo "- **External call**: ${{ needs.detect-changes.outputs.external-call }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Documentation changed**: ${{ needs.detect-changes.outputs.docs-changed }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Ansible files changed**: ${{ needs.detect-changes.outputs.ansible-changed }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Python files changed**: ${{ needs.detect-changes.outputs.python-changed }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Workflow files changed**: ${{ needs.detect-changes.outputs.workflows-changed }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Any code changed**: ${{ needs.detect-changes.outputs.any-code-changed }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ğŸ“Š Job Execution Status" >> $GITHUB_STEP_SUMMARY
          echo "- **Super Linter**: ${{ needs.super-linter.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Security Scan**: ${{ needs.security.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Publishing**: ${{ needs.publish.result || 'skipped (no docs changes or not main branch)' }}" >> $GITHUB_STEP_SUMMARY
