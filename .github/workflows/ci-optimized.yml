---
name: 🚀 CI/CD Pipeline

on:
  # Manual trigger - allows running from GitHub UI
  workflow_dispatch:
    inputs:
      full_scan:
        description: 'Run full codebase scan (not just changed files)'
        required: false
        type: boolean
        default: true
      target_environment:
        description: 'Target environment for publishing'
        required: false
        type: choice
        options:
          - 'production'
          - 'staging'
          - 'development'
        default: 'production'
      dry_run:
        description: 'Dry run mode (no actual publishing)'
        required: false
        type: boolean
        default: false

  # Remote trigger - allows being called by other workflows
  workflow_call:
    inputs:
      full_scan:
        description: 'Run full codebase scan (not just changed files)'
        required: false
        type: boolean
        default: false
      branch_name:
        description: 'Branch name to checkout'
        required: false
        type: string
        default: ''
      target_environment:
        description: 'Target environment for publishing'
        required: false
        type: string
        default: 'production'
      dry_run:
        description: 'Dry run mode (no actual publishing)'
        required: false
        type: boolean
        default: false

  # Remote trigger - allows being called by other workflows
    secrets:
      CONFLUENCE_URL:
        required: false
      CONFLUENCE_USER:
        required: false
      CONFLUENCE_API_TOKEN:
        required: false

jobs:
  # Job to detect what types of files have changed for optimized execution
  detect-changes:
    name: 🔍 Detect File Changes
    runs-on: ubuntu-latest
    outputs:
      docs-changed: ${{ steps.final.outputs.docs-changed }}
      ansible-changed: ${{ steps.final.outputs.ansible-changed }}
      python-changed: ${{ steps.final.outputs.python-changed }}
      workflows-changed: ${{ steps.final.outputs.workflows-changed }}
      any-code-changed: ${{ steps.final.outputs.any-code-changed }}
      external-call: ${{ steps.context.outputs.external-call }}
    steps:
      - name: 📥 Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: ${{ inputs.branch_name || github.ref }}

      - name: 🔍 Check Repository Context
        id: context
        run: |
          echo "Current repository: ${{ github.repository }}"
          echo "Event name: ${{ github.event_name }}"
          echo "Ref: ${{ github.ref }}"
          echo "Ref name: ${{ github.ref_name }}"

          # Detect external calls via workflow_call event
          # When called externally via workflow_call, we assume all changes for simplicity
          if [[ "${{ github.event_name }}" == "workflow_call" ]]; then
            echo "external-call=true" >> $GITHUB_OUTPUT
            echo "🔄 External workflow call detected (workflow_call event) - will assume docs changed"
          else
            echo "external-call=false" >> $GITHUB_OUTPUT
            echo "🏠 Internal repository call - will detect actual changes"
          fi

      - name: 🔍 Detect Changes
        uses: dorny/paths-filter@v2
        if: steps.context.outputs.external-call != 'true'
        id: changes
        with:
          filters: |
            docs:
              - 'docs/**'
              - 'templates/**'
              - 'vars/**'
              - '*.md'
              - '*.j2'
            ansible:
              - 'playbooks/**'
              - 'inventory/**'
              - '*.yml'
              - '*.yaml'
            python:
              - '**/*.py'
              - 'requirements.txt'
            workflows:
              - '.github/**'
            code:
              - '**/*.py'
              - '**/*.yml'
              - '**/*.yaml'
              - '**/*.j2'
              - '**/*.md'

      - name: 🔄 Set Default Outputs
        id: defaults
        if: steps.context.outputs.external-call != 'true' && steps.changes.outcome != 'success'
        run: |
          echo "docs=false" >> $GITHUB_OUTPUT
          echo "ansible=false" >> $GITHUB_OUTPUT
          echo "python=false" >> $GITHUB_OUTPUT
          echo "workflows=false" >> $GITHUB_OUTPUT
          echo "code=false" >> $GITHUB_OUTPUT

      - name: 🔄 Set External Changes
        id: external
        if: steps.context.outputs.external-call == 'true'
        run: |
          echo "🔄 External call detected - assuming all changes"
          echo "docs-changed=true" >> $GITHUB_OUTPUT
          echo "ansible-changed=true" >> $GITHUB_OUTPUT
          echo "python-changed=true" >> $GITHUB_OUTPUT
          echo "workflows-changed=true" >> $GITHUB_OUTPUT
          echo "any-code-changed=true" >> $GITHUB_OUTPUT

      - name: 🔄 Set Internal Changes
        id: internal
        if: steps.context.outputs.external-call != 'true'
        run: |
          echo "🏠 Internal call - using actual change detection"
          echo "docs-changed=${{ steps.changes.outputs.docs || 'false' }}" >> $GITHUB_OUTPUT
          echo "ansible-changed=${{ steps.changes.outputs.ansible || 'false' }}" >> $GITHUB_OUTPUT
          echo "python-changed=${{ steps.changes.outputs.python || 'false' }}" >> $GITHUB_OUTPUT
          echo "workflows-changed=${{ steps.changes.outputs.workflows || 'false' }}" >> $GITHUB_OUTPUT
          echo "any-code-changed=${{ steps.changes.outputs.code || 'false' }}" >> $GITHUB_OUTPUT

      - name: 🔄 Finalize Change Detection
        id: final
        run: |
          # Use outputs from the step that actually ran
          if [[ "${{ steps.context.outputs.external-call }}" == "true" ]]; then
            echo "docs-changed=${{ steps.external.outputs.docs-changed }}" >> $GITHUB_OUTPUT
            echo "ansible-changed=${{ steps.external.outputs.ansible-changed }}" >> $GITHUB_OUTPUT
            echo "python-changed=${{ steps.external.outputs.python-changed }}" >> $GITHUB_OUTPUT
            echo "workflows-changed=${{ steps.external.outputs.workflows-changed }}" >> $GITHUB_OUTPUT
            echo "any-code-changed=${{ steps.external.outputs.any-code-changed }}" >> $GITHUB_OUTPUT
          else
            echo "docs-changed=${{ steps.internal.outputs.docs-changed }}" >> $GITHUB_OUTPUT
            echo "ansible-changed=${{ steps.internal.outputs.ansible-changed }}" >> $GITHUB_OUTPUT
            echo "python-changed=${{ steps.internal.outputs.python-changed }}" >> $GITHUB_OUTPUT
            echo "workflows-changed=${{ steps.internal.outputs.workflows-changed }}" >> $GITHUB_OUTPUT
            echo "any-code-changed=${{ steps.internal.outputs.any-code-changed }}" >> $GITHUB_OUTPUT
          fi

      - name: 📊 Debug Publishing Conditions
        run: |
          echo "## 🔍 Publishing Condition Debug" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Event name**: ${{ github.event_name }}" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Branch reference**: ${{ github.ref }}" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Branch name**: ${{ github.ref_name }}" >> "$GITHUB_STEP_SUMMARY"
          echo "- **External call**: ${{ steps.context.outputs.external-call }}" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Docs changed**: ${{ steps.final.outputs.docs-changed }}" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Full scan input**: ${{ inputs.full_scan }}" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "### Publishing will run if:" >> "$GITHUB_STEP_SUMMARY"
          echo "1. Event is 'workflow_call/workflow_dispatch' ✅: ${{ github.event_name == 'workflow_call' ||
            github.event_name == 'workflow_dispatch' }}" >> "$GITHUB_STEP_SUMMARY"
          echo "2. External call OR branch is main/release/hotfix ✅: ${{ steps.context.outputs.external-call == 'true' ||
            github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/heads/release/') ||
            startsWith(github.ref, 'refs/heads/hotfix/') }}" >> "$GITHUB_STEP_SUMMARY"
          echo "3. Docs changed OR full_scan OR external call ✅: ${{ steps.final.outputs.docs-changed == 'true' ||
            inputs.full_scan == true || steps.context.outputs.external-call == 'true' }}" >> "$GITHUB_STEP_SUMMARY"

  super-linter:
    name: 🔍 Super Linter
    runs-on: ubuntu-latest
    needs: detect-changes
    if: needs.detect-changes.outputs.any-code-changed == 'true' || inputs.full_scan == true

    permissions:
      contents: read
      packages: read
      statuses: write
      security-events: write

    steps:
      - name: 📥 Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: ${{ inputs.branch_name || github.ref }}

      - name: 🔧 Configure Environment
        id: config
        run: |
          # Dynamic branch detection
          if [ "${{ github.event.repository.default_branch }}" != "" ]; then
            echo "default_branch=${{ github.event.repository.default_branch }}" >> $GITHUB_OUTPUT
          else
            echo "default_branch=main" >> $GITHUB_OUTPUT
          fi

      - name: 🔍 Verify Linter Configuration Files
        run: |
          echo "🔍 Checking for linter configuration files..."

          # Check for .ansible-lint
          if [ -f ".ansible-lint" ]; then
            echo "✅ .ansible-lint found ($(wc -l < .ansible-lint) lines)"
            echo "📄 .ansible-lint preview:"
            head -5 .ansible-lint
          else
            echo "❌ .ansible-lint not found"
          fi

          # Check for .yamllint
          if [ -f ".yamllint" ]; then
            echo "✅ .yamllint found ($(wc -l < .yamllint) lines)"
          else
            echo "❌ .yamllint not found"
          fi

          # Check for markdownlint config (prefer .json)
          if [ -f ".markdownlint.json" ]; then
            echo "✅ .markdownlint.json found ($(wc -l < .markdownlint.json) lines)"
          elif [ -f ".markdownlint.yml" ]; then
            echo "✅ .markdownlint.yml found ($(wc -l < .markdownlint.yml) lines)"
          else
            echo "❌ No markdownlint config found"
          fi

          # Check for any conflicting config files
          if [ -f ".markdownlint.json" ] && [ -f ".markdownlint.yml" ]; then
            echo "⚠️  Both .markdownlint.json and .markdownlint.yml found - Super Linter will prefer .json"
          fi

          echo "🎯 Super Linter will auto-detect all configuration files at repo root"

      - name: 🔍 Detect Files for Validation
        id: detect-files
        run: |
          echo "🔍 Detecting file types for intelligent validation..."

          # Check for different file types and set outputs (with proper quoting)
          yaml_files=$(find . -type f \( -name "*.yml" -o -name "*.yaml" \) ! -path "./.git/*" | wc -l)
          ansible_files=$(find . -type f \( -path "./playbooks/*" -o -path "./roles/*" \
            -o -name "site.yml" -o -name "playbook*.yml" -o -name "playbook*.yaml" \) | wc -l)
          python_files=$(find . -type f -name "*.py" ! -path "./.git/*" ! -path "./.venv/*" ! -path "./venv/*" | wc -l)
          markdown_files=$(find . -type f -name "*.md" ! -path "./.git/*" | wc -l)
          shell_files=$(find . -type f -name "*.sh" ! -path "./.git/*" | wc -l)
          json_files=$(find . -type f -name "*.json" ! -path "./.git/*" ! -path "./node_modules/*" | wc -l)
          github_actions_files=$(find .github/workflows -type f \( -name "*.yml" -o -name "*.yaml" \) 2>/dev/null |
            wc -l)

          # Set outputs
          echo "yaml_files=$yaml_files" >> $GITHUB_OUTPUT
          echo "ansible_files=$ansible_files" >> $GITHUB_OUTPUT
          echo "python_files=$python_files" >> $GITHUB_OUTPUT
          echo "markdown_files=$markdown_files" >> $GITHUB_OUTPUT
          echo "shell_files=$shell_files" >> $GITHUB_OUTPUT
          echo "json_files=$json_files" >> $GITHUB_OUTPUT
          echo "github_actions_files=$github_actions_files" >> $GITHUB_OUTPUT

          # Output summary
          echo "🎯 File detection summary:"
          echo "  - YAML files found: $yaml_files"
          echo "  - Ansible files found: $ansible_files"
          echo "  - Python files found: $python_files"
          echo "  - Markdown files found: $markdown_files"
          echo "  - Shell files found: $shell_files"
          echo "  - JSON files found: $json_files"
          echo "  - GitHub Actions files found: $github_actions_files"
          echo ""
          echo "📋 Linters that will be enabled based on file detection:"
          [ "$yaml_files" -gt 0 ] && echo "  ✅ YAML/yamllint"
          [ "$ansible_files" -gt 0 ] && echo "  ✅ Ansible/ansible-lint"
          [ "$markdown_files" -gt 0 ] && echo "  ✅ Markdown/markdownlint"
          [ "$python_files" -gt 0 ] && echo "  ✅ Python/flake8"
          [ "$shell_files" -gt 0 ] && echo "  ✅ Shell/ShellCheck"
          [ "$json_files" -gt 0 ] && echo "  ✅ JSON/jsonlint"
          [ "$github_actions_files" -gt 0 ] && echo "  ✅ GitHub Actions/actionlint"

      - name: 🔍 Verify Configuration Files
        run: |
          echo "🔍 Verifying Super Linter configuration..."
          echo "Static configuration file contents:"
          cat .github/super-linter.env
          echo ""
          echo "🔍 Verifying yamllint configuration:"
          if [ -f ".yamllint" ]; then
            echo "✅ .yamllint file exists"
            echo "📄 .yamllint line-length configuration:"
            grep -A 10 "line-length:" .yamllint || echo "❌ line-length not found in .yamllint"
          else
            echo "❌ .yamllint file not found"
          fi

      - name: 🔍 Run Super Linter
        id: super-linter
        uses: super-linter/super-linter@v5
        env:
          # Use static configuration file
          ENV_FILE: .github/super-linter.env

          # GitHub token (required for API access)
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

          # Branch and validation settings
          DEFAULT_BRANCH: ${{ steps.config.outputs.default_branch }}
          VALIDATE_ALL_CODEBASE: ${{ inputs.full_scan == true }}

      - name: � Enhanced Super Linter Analysis
        if: always()
        id: enhanced-analysis
        run: |
          echo "::group::🔍 Enhanced Super Linter Analysis"

          # Run comprehensive analysis that includes autofix detection and shell script analysis
          echo "� Running enhanced code quality analysis with shell script tracking..."

          # Enhanced analysis including shell script issues
          python3 << 'EOF'
          import os
          import json
          import glob
          import sys
          import subprocess
          from pathlib import Path

          def analyze_shell_scripts():
              """Analyze shell scripts specifically for ShellCheck issues"""
              shell_files = []
              # Find shell script files
              for pattern in ['**/*.sh', '**/*.bash']:
                  shell_files.extend(glob.glob(pattern, recursive=True))

              # Also check workflow files for embedded shell scripts
              workflow_files = glob.glob('.github/workflows/*.yml', recursive=True)

              print(f"🐚 Found {len(shell_files)} shell script files for analysis")
              print(f"📋 Found {len(workflow_files)} workflow files to check for shell scripts")

              shell_issues = 0
              shell_fixes = 0

              # Analyze each shell file with ShellCheck if available
              for shell_file in shell_files:
                  if os.path.exists(shell_file):
                      try:
                          # Use shellcheck to analyze the file
                          result = subprocess.run(['shellcheck', '-f', 'json', shell_file],
                                                capture_output=True, text=True, timeout=30)
                          if result.stdout:
                              issues = json.loads(result.stdout)
                              file_issues = len(issues)
                              shell_issues += file_issues
                              # Count auto-fixable issues
                              auto_fixable = sum(1 for issue in issues
                                               if issue.get('code') in [
                                                   2086,  # Unquoted variables
                                                   2048,  # Invalid number
                                                   2006,  # Use $(...) instead of backticks
                                                   2035,  # Missing quotes
                                                   2129,  # Multiple redirections
                                               ])
                              shell_fixes += auto_fixable
                              if file_issues > 0:
                                  print(f"  🐚 {shell_file}: {file_issues} issues, {auto_fixable} auto-fixable")
                      except (json.JSONDecodeError, subprocess.SubprocessError, subprocess.TimeoutExpired, FileNotFoundError) as e:
                          print(f"  ℹ️  ShellCheck analysis skipped for {shell_file}: {type(e).__name__}")

              return shell_issues, shell_fixes

          def get_super_linter_results():
              """Parse Super Linter output for comprehensive analysis"""
              results = {
                  'yaml_issues': 0, 'yaml_fixes': 0,
                  'ansible_issues': 0, 'ansible_fixes': 0,
                  'python_issues': 0, 'python_fixes': 0,
                  'shell_issues': 0, 'shell_fixes': 0,
                  'markdown_issues': 0, 'markdown_fixes': 0,
                  'json_issues': 0, 'json_fixes': 0,
                  'total_issues': 0, 'total_fixes': 0
              }

              # Analyze shell scripts specifically
              shell_issues, shell_fixes = analyze_shell_scripts()
              results['shell_issues'] = shell_issues
              results['shell_fixes'] = shell_fixes

              # Check for Super Linter output files
              report_dirs = ["super-linter.report", "./", "reports"]
              for report_dir in report_dirs:
                  if os.path.exists(report_dir):
                      print(f"📁 Checking directory: {report_dir}")
                      for report_file in glob.glob(f"{report_dir}/**/*.log", recursive=True):
                          print(f"  📄 Found report: {os.path.basename(report_file)}")
                      for report_file in glob.glob(f"{report_dir}/**/*.tap", recursive=True):
                          print(f"  📊 Found TAP report: {os.path.basename(report_file)}")

              # Calculate totals
              results['total_issues'] = sum(v for k, v in results.items() if k.endswith('_issues'))
              results['total_fixes'] = sum(v for k, v in results.items() if k.endswith('_fixes'))

              # Calculate health score
              max_possible_score = 100
              if results['total_issues'] > 0:
                  # Deduct points based on issues found
                  deduction = min(results['total_issues'] * 2, 80)  # Max 80 point deduction
                  health_score = max(max_possible_score - deduction, 20)  # Min score of 20
              else:
                  health_score = max_possible_score

              results['overall_health_score'] = health_score

              return results

          # Main analysis
          print("🔍 Starting enhanced Super Linter analysis with shell script tracking...")
          results = get_super_linter_results()

          # Output results for GitHub Actions
          print(f"📊 Analysis Results:")
          print(f"  🐚 Shell issues: {results['shell_issues']} (fixes: {results['shell_fixes']})")
          print(f"  📄 YAML issues: {results['yaml_issues']} (fixes: {results['yaml_fixes']})")
          print(f"  🎭 Ansible issues: {results['ansible_issues']} (fixes: {results['ansible_fixes']})")
          print(f"  🐍 Python issues: {results['python_issues']} (fixes: {results['python_fixes']})")
          print(f"  📝 Markdown issues: {results['markdown_issues']} (fixes: {results['markdown_fixes']})")
          print(f"  📋 JSON issues: {results['json_issues']} (fixes: {results['json_fixes']})")
          print(f"  🏥 Overall health score: {results['overall_health_score']}/100")

          # Set GitHub Actions outputs
          autofix_needed = "true" if results['total_fixes'] > 0 else "false"

          # Write outputs to GitHub Actions
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"autofix_needed={autofix_needed}\n")
              f.write(f"total_fixes={results['total_fixes']}\n")
              f.write(f"total_issues={results['total_issues']}\n")
              f.write(f"shell_issues={results['shell_issues']}\n")
              f.write(f"shell_fixes={results['shell_fixes']}\n")
              f.write(f"yaml_issues={results['yaml_issues']}\n")
              f.write(f"yaml_fixes={results['yaml_fixes']}\n")
              f.write(f"ansible_issues={results['ansible_issues']}\n")
              f.write(f"ansible_fixes={results['ansible_fixes']}\n")
              f.write(f"python_issues={results['python_issues']}\n")
              f.write(f"python_fixes={results['python_fixes']}\n")
              f.write(f"markdown_issues={results['markdown_issues']}\n")
              f.write(f"markdown_fixes={results['markdown_fixes']}\n")
              f.write(f"json_issues={results['json_issues']}\n")
              f.write(f"json_fixes={results['json_fixes']}\n")
              f.write(f"overall_health_score={results['overall_health_score']}\n")

          print("✅ Enhanced analysis with shell script tracking completed")
          EOF

          echo "✅ Enhanced analysis completed"
          echo "::endgroup::"
        working-directory: ${{ github.workspace }}

      - name: 📊 Enhanced Analysis Summary
        if: always()
        run: |
          {
            echo "## 🔍 Enhanced Super Linter Analysis Results"
            echo ""
            echo "### 📋 Code Quality Analysis"
            echo "- **🐚 Shell script issues**: ${{ steps.enhanced-analysis.outputs.shell_issues || '0' }} (auto-fixable: ${{ steps.enhanced-analysis.outputs.shell_fixes || '0' }})"
            echo "- **📄 YAML issues**: ${{ steps.enhanced-analysis.outputs.yaml_issues || '0' }} (auto-fixable: ${{ steps.enhanced-analysis.outputs.yaml_fixes || '0' }})"
            echo "- **🎭 Ansible issues**: ${{ steps.enhanced-analysis.outputs.ansible_issues || '0' }} (auto-fixable: ${{ steps.enhanced-analysis.outputs.ansible_fixes || '0' }})"
            echo "- **🐍 Python issues**: ${{ steps.enhanced-analysis.outputs.python_issues || '0' }} (auto-fixable: ${{ steps.enhanced-analysis.outputs.python_fixes || '0' }})"
            echo "- **📝 Markdown issues**: ${{ steps.enhanced-analysis.outputs.markdown_issues || '0' }} (auto-fixable: ${{ steps.enhanced-analysis.outputs.markdown_fixes || '0' }})"
            echo "- **📋 JSON issues**: ${{ steps.enhanced-analysis.outputs.json_issues || '0' }} (auto-fixable: ${{ steps.enhanced-analysis.outputs.json_fixes || '0' }})"
            echo ""
            echo "### 🏥 Overall Health Score"
            echo "**Score**: ${{ steps.enhanced-analysis.outputs.overall_health_score || 'N/A' }}/100"
            echo ""
            echo "### 🤖 Auto-fix Status"
            if [ "${{ steps.enhanced-analysis.outputs.autofix_needed }}" = "true" ]; then
              echo "✅ **Auto-fixes available**: ${{ steps.enhanced-analysis.outputs.total_fixes }} potential fixes detected"
            else
              echo "🎉 **No auto-fixes needed**: Code quality is excellent!"
            fi
            echo ""
            echo "### 🔍 Analysis Coverage"
            echo "- Shell script analysis with ShellCheck integration"
            echo "- Multi-language linting with Super Linter"
            echo "- Auto-fixable issue detection and classification"
            echo "- Health score calculation based on issue severity"
          } >> "$GITHUB_STEP_SUMMARY"

      - name: 🤖 Apply Auto-fixes
        if: always()
        id: apply-fixes
        run: |
          echo "🤖 Applying auto-fixes to detected issues..."
          
          # Initialize counters
          total_fixes=0
          yaml_fixes=0
          python_fixes=0
          shell_fixes=0
          markdown_fixes=0
          json_fixes=0
          
          # Python auto-fixes with Black and isort
          echo "🐍 Applying Python auto-fixes..."
          if command -v black >/dev/null 2>&1; then
            if find . -name "*.py" -not -path "./.git/*" -not -path "./.venv/*" -not -path "./venv/*" -not -path "./node_modules/*" | xargs black --check --quiet 2>/dev/null; then
              echo "  ✅ Python files already formatted correctly"
            else
              echo "  � Formatting Python files with Black..."
              python_black_count=$(find . -name "*.py" -not -path "./.git/*" -not -path "./.venv/*" -not -path "./venv/*" -not -path "./node_modules/*" | xargs black --diff --quiet 2>&1 | grep -c "would reformat" || echo "0")
              find . -name "*.py" -not -path "./.git/*" -not -path "./.venv/*" -not -path "./venv/*" -not -path "./node_modules/*" | xargs black
              python_fixes=$((python_fixes + python_black_count))
              total_fixes=$((total_fixes + python_black_count))
              echo "    Applied $python_black_count Black formatting fixes"
            fi
          fi
          
          if command -v isort >/dev/null 2>&1; then
            echo "  🔧 Sorting Python imports with isort..."
            python_isort_count=$(find . -name "*.py" -not -path "./.git/*" -not -path "./.venv/*" -not -path "./venv/*" -not -path "./node_modules/*" | xargs isort --check-only --diff 2>&1 | grep -c "Fixing" || echo "0")
            find . -name "*.py" -not -path "./.git/*" -not -path "./.venv/*" -not -path "./venv/*" -not -path "./node_modules/*" | xargs isort
            python_fixes=$((python_fixes + python_isort_count))
            total_fixes=$((total_fixes + python_isort_count))
            echo "    Applied $python_isort_count import sorting fixes"
          fi
          
          # YAML auto-fixes (basic formatting)
          echo "📄 Checking YAML files..."
          yaml_count=0
          for file in $(find . -name "*.yml" -o -name "*.yaml" | grep -v ".git"); do
            if yamllint "$file" >/dev/null 2>&1; then
              continue
            else
              # Simple YAML fixes: trailing whitespace
              if sed -i 's/[[:space:]]*$//' "$file"; then
                yaml_count=$((yaml_count + 1))
              fi
            fi
          done
          yaml_fixes=$yaml_count
          total_fixes=$((total_fixes + yaml_count))
          echo "  Applied $yaml_count YAML formatting fixes"
          
          # Shell script auto-fixes with shfmt
          echo "🐚 Applying shell script auto-fixes..."
          if command -v shfmt >/dev/null 2>&1; then
            shell_count=0
            for file in $(find . -name "*.sh" | grep -v ".git"); do
              if shfmt -d "$file" >/dev/null 2>&1; then
                shfmt -w "$file"
                shell_count=$((shell_count + 1))
              fi
            done
            shell_fixes=$shell_count
            total_fixes=$((total_fixes + shell_count))
            echo "  Applied $shell_count shell script formatting fixes"
          fi
          
          # JSON auto-fixes
          echo "📋 Applying JSON auto-fixes..."
          json_count=0
          for file in $(find . -name "*.json" | grep -v ".git" | grep -v "node_modules" | grep -v "package-lock.json"); do
            if python3 -m json.tool "$file" > "${file}.tmp" 2>/dev/null; then
              if ! diff -q "$file" "${file}.tmp" >/dev/null 2>&1; then
                mv "${file}.tmp" "$file"
                json_count=$((json_count + 1))
              else
                rm -f "${file}.tmp"
              fi
            else
              rm -f "${file}.tmp"
            fi
          done
          json_fixes=$json_count
          total_fixes=$((total_fixes + json_count))
          echo "  Applied $json_count JSON formatting fixes"
          
          # Set outputs for the commit step
          echo "autofix_needed=$([ $total_fixes -gt 0 ] && echo true || echo false)" >> $GITHUB_OUTPUT
          echo "total_fixes=$total_fixes" >> $GITHUB_OUTPUT
          echo "python_fixes=$python_fixes" >> $GITHUB_OUTPUT
          echo "yaml_fixes=$yaml_fixes" >> $GITHUB_OUTPUT
          echo "shell_fixes=$shell_fixes" >> $GITHUB_OUTPUT
          echo "json_fixes=$json_fixes" >> $GITHUB_OUTPUT
          echo "markdown_fixes=0" >> $GITHUB_OUTPUT
          echo "ansible_fixes=0" >> $GITHUB_OUTPUT
          
          echo "🎯 Auto-fix summary: $total_fixes total fixes applied"

      - name: 💾 Commit Auto-fixes
        if: steps.apply-fixes.outputs.autofix_needed == 'true'
        run: |
          echo "💾 Committing auto-fixes..."

          # Configure git (use GitHub Actions bot)
          git config --local user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"

          # Add all changes
          git add .

          # Check if there are actually changes to commit
          if git diff --staged --quiet; then
            echo "ℹ️ No changes to commit after autofix attempts"
            exit 0
          fi

          # Create detailed commit message
          commit_msg="🤖 Auto-fix: Applied ${{ steps.apply-fixes.outputs.total_fixes }} linting fixes

          Auto-fixes applied by formatters:
          - Shell fixes: ${{ steps.apply-fixes.outputs.shell_fixes }}
          - YAML fixes: ${{ steps.apply-fixes.outputs.yaml_fixes }}
          - Python fixes: ${{ steps.apply-fixes.outputs.python_fixes }}
          - Markdown fixes: ${{ steps.apply-fixes.outputs.markdown_fixes }}
          - JSON fixes: ${{ steps.apply-fixes.outputs.json_fixes }}

          Automated by: ${{ github.workflow }} #${{ github.run_number }}
          Event: ${{ github.event_name }}
          Health Score: ${{ steps.enhanced-analysis.outputs.overall_health_score }}/100"

          # Commit changes
          git commit -m "$commit_msg"

          # Push changes back to the branch
          git push origin ${{ github.ref_name }}

          echo "✅ Auto-fixes committed and pushed!"

      - name: ⚠️ Auto-fix Status
        if: steps.enhanced-analysis.outputs.autofix_needed == 'true'
        run: |
          echo "🔍 Auto-fixes applied successfully!"
          echo "� Summary of fixes:"
          echo "  - Shell fixes: ${{ steps.enhanced-analysis.outputs.shell_fixes }}"
          echo "  - YAML fixes: ${{ steps.enhanced-analysis.outputs.yaml_fixes }}"
          echo "  - Ansible fixes: ${{ steps.enhanced-analysis.outputs.ansible_fixes }}"
          echo "  - Python fixes: ${{ steps.enhanced-analysis.outputs.python_fixes }}"
          echo "  - Markdown fixes: ${{ steps.enhanced-analysis.outputs.markdown_fixes }}"
          echo "  - JSON fixes: ${{ steps.enhanced-analysis.outputs.json_fixes }}"
          echo ""
          echo "✅ Changes have been committed and pushed back to the branch"

      - name: 📤 Upload Super Linter Logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: super-linter-logs-${{ github.run_id }}
          path: |
            super-linter.log
            github-super-linter.log
            .github/super-linter.env
          retention-days: 30
          if-no-files-found: ignore

      - name: 📤 Upload Static Configuration (Debug)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: super-linter-config-${{ github.run_id }}
          path: .github/super-linter.env
          retention-days: 7
          if-no-files-found: warn

  # Enhanced security scanning - only runs when code changes
  security:
    name: 🛡️ Security Scan
    runs-on: ubuntu-latest
    needs: detect-changes
    if: needs.detect-changes.outputs.any-code-changed == 'true' || inputs.full_scan == true
    permissions:
      actions: read
      contents: read
      security-events: write
    steps:
      - name: 📥 Checkout Code
        uses: actions/checkout@v4
        with:
          ref: ${{ inputs.branch_name || github.ref }}

      - name: 🔍 Run DevSkim Scanner
        uses: microsoft/DevSkim-Action@v1

      - name: 📤 Upload DevSkim SARIF
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: devskim-results.sarif

      - name: ☢️ Run Trivy Vulnerability Scanner
        uses: aquasecurity/trivy-action@0.32.0
        with:
          scan-type: 'fs'
          ignore-unfixed: true
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH'

      - name: 📤 Upload Trivy Scan Results
        uses: actions/upload-artifact@v4
        with:
          name: trivy-results
          path: trivy-results.sarif

      - name: 🔐 Advanced Secret Detection
        run: |
          echo "🔍 Running security validation..."

          # Check for potential secrets (excluding false positives)
          echo "Checking for potential hardcoded secrets..."
          if grep -rE "(password|secret|api_key|auth_token|private_key):\s*['\"]?[A-Za-z0-9+/=]{10,}" . \
             --include="*.yml" --include="*.yaml" --include="*.py" --include="*.sh" \
             --exclude-dir=.git --exclude-dir=.github \
             --exclude="*example*" --exclude="*template*" \
             | grep -v "YOUR_.*_HERE\|test:test\|example\|template\|#.*token\|#.*secret\|README"; then
            echo "⚠️ Potential secrets found - please review"
            exit 1
          else
            echo "✅ No obvious secrets detected"
          fi

      - name: 🔒 File Permissions Check
        run: |
          echo "🔒 Checking file permissions..."

          # Check for world-writable files
          if find . -name "*.yml" -o -name "*.yaml" -o -name "*.py" -o -name "*.sh" |
            xargs ls -la | grep "^-.......rw"; then
            echo "❌ World-writable files found"
            find . -name "*.yml" -o -name "*.yaml" -o -name "*.py" -o -name "*.sh" | xargs ls -la | grep "^-.......rw"
            exit 1
          else
            echo "✅ File permissions look secure"
          fi

      - name: 🛡️ Security Summary
        if: always()
        run: |
          {
            echo "## 🛡️ Security Validation Summary"
            echo "✅ **DevSkim scan completed**"
            echo "✅ **Secret detection completed**"
            echo "✅ **File permissions checked**"
          } >> "$GITHUB_STEP_SUMMARY"

  # Publish job - runs on main/release/hotfix branches when docs change, regardless of linting results
  publish:
    name: 🚀 Publish to Confluence

    # Use relative path for better compatibility with external calls
    uses: ./.github/workflows/publish-docs.yml
    with:
      target_environment: ${{ inputs.target_environment || 'production' }}
      dry_run: ${{ inputs.dry_run || false }}
      full_scan: ${{ inputs.full_scan }}

    # Pass the required secrets to the reusable workflow.
    secrets:
      CONFLUENCE_URL: ${{ secrets.CONFLUENCE_URL }}
      CONFLUENCE_USER: ${{ secrets.CONFLUENCE_USER }}
      CONFLUENCE_API_TOKEN: ${{ secrets.CONFLUENCE_API_TOKEN }}

  # Comprehensive report - always runs to provide summary
  comprehensive-report:
    name: 📊 Generate Comprehensive Report
    # This job runs after all checks are complete, including publish which now runs in parallel
    needs: [super-linter, security, detect-changes, publish]
    if: always()  # Always run to report on success or failure
    runs-on: ubuntu-latest
    steps:
      - name: � Checkout Code (for scripts)
        uses: actions/checkout@v4
        with:
          ref: ${{ inputs.branch_name || github.ref }}

      - name: �📥 Download all artifacts
        uses: actions/download-artifact@v4
        with:
          # Download all artifacts into a 'reports' directory
          path: ./reports
        continue-on-error: true

      - name: 🐍 Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'
          cache: 'pip'

      - name: 📦 Install Dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi

      - name: 📝 Generate SARIF Security Reports Summary
        if: always()
        run: |
          python3 << 'EOF'
          import json
          import os
          import glob
          import sys

          def parse_sarif(file_path):
              if not os.path.exists(file_path):
                  return '| N/A | No report found | This scan may have failed to produce a report. | N/A |\n', 0

              report_md = ''
              results = {}
              total_issues = 0

              try:
                  with open(file_path, 'r', encoding='utf-8') as f:
                      sarif_data = json.load(f)
                      for run in sarif_data.get('runs', []):
                          for result in run.get('results', []):
                              total_issues += 1
                              rule_id = result.get('ruleId', 'unknown')
                              message = result.get('message', {}).get('text', 'No description available')
                              level = result.get('level', 'warning').upper()
                              key = (level, rule_id, message)
                              results[key] = results.get(key, 0) + 1
              except (json.JSONDecodeError, KeyError, TypeError) as e:
                  return f'| ERROR | PARSE_ERROR | Failed to parse SARIF file: {str(e)} | N/A |\n', 0

              if not results:
                  report_md += '| ✅ | None | No issues found! | 0 |\n'
              else:
                  for (level, rule, msg), count in sorted(results.items()):
                      msg_escaped = msg.replace('|', '\\|').replace('\n', ' ').replace('\r', '').replace('`', '\\`')
                      report_md += f'| {level} | \\`{rule}\\` | {msg_escaped} | {count} |\n'

              return report_md, total_issues

          # Main execution
          summary_file = os.getenv('GITHUB_STEP_SUMMARY')
          if not summary_file:
              print('ERROR: GITHUB_STEP_SUMMARY environment variable not set', file=sys.stderr)
              sys.exit(1)

          reports_dir = './reports'
          overall_total = 0

          # Dynamically discover all SARIF reports
          all_reports = {}
          for p in glob.glob(os.path.join(reports_dir, '**', '*.sarif'), recursive=True):
              name = os.path.basename(os.path.dirname(p)).replace('-', ' ').replace('_', ' ').title()
              if name:
                  all_reports[name] = p

          print(f'Processing {len(all_reports)} report types...')
          print(f'Reports directory: {reports_dir}')
          print(f'Summary will be written to: {summary_file}')

          # Check if reports directory exists
          if not os.path.exists(reports_dir):
              print(f'WARNING: Reports directory {reports_dir} does not exist')
              print('This is normal if no artifacts were generated or downloaded.')
              with open(summary_file, 'a') as f:
                  f.write('## 📊 Security & Quality Reports Summary\n\n')
                  f.write('### ⚠️ No Reports Found\n\n')
                  f.write('The reports directory was not found. This may indicate:\n')
                  f.write('- No artifacts were generated by the scanning jobs\n')
                  f.write('- All scans passed without issues (no SARIF files created)\n')
                  f.write('- Artifact download failed or was skipped\n\n')
                  f.write('**This is often normal** - it means no security or linting issues were detected!\n\n')
              print('✅ Summary generated with no reports found message')
              sys.exit(0)

          # Check if directory is empty
          all_files = glob.glob(os.path.join(reports_dir, '**', '*'), recursive=True)
          report_files = [f for f in all_files if f.endswith('.sarif')]

          if not report_files:
              print(f'INFO: Reports directory exists but contains no SARIF files')
              print(f'Found {len(all_files)} total files in reports directory')
              with open(summary_file, 'a') as f:
                  f.write('## 📊 Security & Quality Reports Summary\n\n')
                  f.write('### ✅ No Issues Found\n\n')
                  f.write('Reports directory exists but no SARIF reports were found.\n')
                  f.write('This typically means all security and linting scans passed successfully!\n\n')
                  if len(all_files) > 0:
                      f.write(f'📁 **Files found in reports directory**: {len(all_files)}\n')
                      f.write('(These may be log files or other artifacts)\n\n')
              print('✅ Summary generated with no issues found message')
              sys.exit(0)

          # Process each report
          with open(summary_file, 'a') as f:
              f.write('## 📊 Security & Quality Reports Summary\n\n')

          for tool_name, report_path in all_reports.items():
              print(f'Processing {tool_name} report: {report_path}')

              with open(summary_file, 'a') as f:
                  f.write(f'### {tool_name} Report\n')
                  f.write('| Severity | Rule ID | Description | Total |\n')
                  f.write('|----------|---------|-------------|-------|\n')

              report_content, issue_count = parse_sarif(report_path)
              overall_total += issue_count

              with open(summary_file, 'a') as f:
                  f.write(report_content)
                  f.write('\n')

              print(f'  - Found {issue_count} issues in {tool_name}')

          # Final summary
          with open(summary_file, 'a') as f:
              f.write('\n---\n\n')
              if overall_total == 0:
                  f.write('### 🎉 Excellent! No Issues Found\n\n')
                  f.write('All security and quality scans completed successfully with no issues detected.\n')
              else:
                  f.write(f'### 📊 Total Issues Found: {overall_total}\n\n')
                  f.write('Please review the individual reports above for detailed information.\n')

          print(f'✅ Summary generation complete. Total issues across all tools: {overall_total}')
          EOF

      - name: 📊 Execution Summary
        if: always()
        run: |
          {
            echo "## 🚀 Workflow Execution Summary"
            echo ""
            echo "### 🔍 Change Detection Results"
            echo "- **Repository**: ${{ github.repository }}"
            echo "- **External call**: ${{ needs.detect-changes.outputs.external-call }}"
            echo "- **Documentation changed**: ${{ needs.detect-changes.outputs.docs-changed }}"
            echo "- **Ansible files changed**: ${{ needs.detect-changes.outputs.ansible-changed }}"
            echo "- **Python files changed**: ${{ needs.detect-changes.outputs.python-changed }}"
            echo "- **Workflow files changed**: ${{ needs.detect-changes.outputs.workflows-changed }}"
            echo "- **Any code changed**: ${{ needs.detect-changes.outputs.any-code-changed }}"
            echo ""
            echo "### 📊 Job Execution Status"
            echo "- **Super Linter**: ${{ needs.super-linter.result }}"
            echo "- **Security Scan**: ${{ needs.security.result }}"
            echo "- **Publishing**: ${{ needs.publish.result || 'skipped (no docs changes or not main branch)' }}"
          } >> "$GITHUB_STEP_SUMMARY"
