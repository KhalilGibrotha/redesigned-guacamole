---
name: üöÄ CI/CD Pipeline

on:
  # Manual trigger - allows running from GitHub UI
  workflow_dispatch:
    inputs:
      full_scan:
        description: 'Run full codebase scan (not just changed files)'
        required: false
        type: boolean
        default: true
      auto_fix:
        description: 'Apply auto-fixes to detected issues'
        required: false
        type: boolean
        default: true
      target_environment:
        description: 'Target environment for publishing'
        required: false
        type: choice
        options:
          - 'production'
          - 'staging'
          - 'development'
        default: 'production'
      dry_run:
        description: 'Dry run mode (no actual publishing)'
        required: false
        type: boolean
        default: false

  # Remote trigger - allows being called by other workflows
  workflow_call:
    inputs:
      full_scan:
        description: 'Run full codebase scan (not just changed files)'
        required: false
        type: boolean
        default: false
      auto_fix:
        description: 'Apply auto-fixes to detected issues'
        required: false
        type: boolean
        default: true
      branch_name:
        description: 'Branch name to checkout'
        required: false
        type: string
        default: ''
      target_environment:
        description: 'Target environment for publishing'
        required: false
        type: string
        default: 'production'
      dry_run:
        description: 'Dry run mode (no actual publishing)'
        required: false
        type: boolean
        default: false

  # Remote trigger - allows being called by other workflows
    secrets:
      CONFLUENCE_URL:
        required: false
      CONFLUENCE_USER:
        required: false
      CONFLUENCE_API_TOKEN:
        required: false

jobs:
  # Job to detect what types of files have changed for optimized execution
  detect-changes:
    name: üîç Detect File Changes
    runs-on: ubuntu-latest
    outputs:
      docs-changed: ${{ steps.final.outputs.docs-changed }}
      ansible-changed: ${{ steps.final.outputs.ansible-changed }}
      python-changed: ${{ steps.final.outputs.python-changed }}
      workflows-changed: ${{ steps.final.outputs.workflows-changed }}
      any-code-changed: ${{ steps.final.outputs.any-code-changed }}
      external-call: ${{ steps.context.outputs.external-call }}
    steps:
      - name: üì• Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: ${{ inputs.branch_name || github.ref }}

      - name: üîç Check Repository Context
        id: context
        run: |
          echo "Current repository: ${{ github.repository }}"
          echo "Event name: ${{ github.event_name }}"
          echo "Ref: ${{ github.ref }}"
          echo "Ref name: ${{ github.ref_name }}"

          # Detect external calls via workflow_call event
          # When called externally via workflow_call, we assume all changes for simplicity
          if [[ "${{ github.event_name }}" == "workflow_call" ]]; then
            echo "external-call=true" >> $GITHUB_OUTPUT
            echo "üîÑ External workflow call detected (workflow_call event) - will assume docs changed"
          else
            echo "external-call=false" >> $GITHUB_OUTPUT
            echo "üè† Internal repository call - will detect actual changes"
          fi

      - name: üîç Detect Changes
        uses: dorny/paths-filter@v2
        if: steps.context.outputs.external-call != 'true'
        id: changes
        with:
          filters: |
            docs:
              - 'docs/**'
              - 'templates/**'
              - 'vars/**'
              - '*.md'
              - '*.j2'
            ansible:
              - 'playbooks/**'
              - 'inventory/**'
              - '*.yml'
              - '*.yaml'
            python:
              - '**/*.py'
              - 'requirements.txt'
            workflows:
              - '.github/**'
            code:
              - '**/*.py'
              - '**/*.yml'
              - '**/*.yaml'
              - '**/*.j2'
              - '**/*.md'

      - name: üîÑ Set Default Outputs
        id: defaults
        if: steps.context.outputs.external-call != 'true' && steps.changes.outcome != 'success'
        run: |
          echo "docs=false" >> $GITHUB_OUTPUT
          echo "ansible=false" >> $GITHUB_OUTPUT
          echo "python=false" >> $GITHUB_OUTPUT
          echo "workflows=false" >> $GITHUB_OUTPUT
          echo "code=false" >> $GITHUB_OUTPUT

      - name: üîÑ Set External Changes
        id: external
        if: steps.context.outputs.external-call == 'true'
        run: |
          echo "üîÑ External call detected - assuming all changes"
          echo "docs-changed=true" >> $GITHUB_OUTPUT
          echo "ansible-changed=true" >> $GITHUB_OUTPUT
          echo "python-changed=true" >> $GITHUB_OUTPUT
          echo "workflows-changed=true" >> $GITHUB_OUTPUT
          echo "any-code-changed=true" >> $GITHUB_OUTPUT

      - name: üîÑ Set Internal Changes
        id: internal
        if: steps.context.outputs.external-call != 'true'
        run: |
          echo "üè† Internal call - using actual change detection"
          echo "docs-changed=${{ steps.changes.outputs.docs || 'false' }}" >> $GITHUB_OUTPUT
          echo "ansible-changed=${{ steps.changes.outputs.ansible || 'false' }}" >> $GITHUB_OUTPUT
          echo "python-changed=${{ steps.changes.outputs.python || 'false' }}" >> $GITHUB_OUTPUT
          echo "workflows-changed=${{ steps.changes.outputs.workflows || 'false' }}" >> $GITHUB_OUTPUT
          echo "any-code-changed=${{ steps.changes.outputs.code || 'false' }}" >> $GITHUB_OUTPUT

      - name: üîÑ Finalize Change Detection
        id: final
        run: |
          # Use outputs from the step that actually ran
          if [[ "${{ steps.context.outputs.external-call }}" == "true" ]]; then
            echo "docs-changed=${{ steps.external.outputs.docs-changed }}" >> $GITHUB_OUTPUT
            echo "ansible-changed=${{ steps.external.outputs.ansible-changed }}" >> $GITHUB_OUTPUT
            echo "python-changed=${{ steps.external.outputs.python-changed }}" >> $GITHUB_OUTPUT
            echo "workflows-changed=${{ steps.external.outputs.workflows-changed }}" >> $GITHUB_OUTPUT
            echo "any-code-changed=${{ steps.external.outputs.any-code-changed }}" >> $GITHUB_OUTPUT
          else
            echo "docs-changed=${{ steps.internal.outputs.docs-changed }}" >> $GITHUB_OUTPUT
            echo "ansible-changed=${{ steps.internal.outputs.ansible-changed }}" >> $GITHUB_OUTPUT
            echo "python-changed=${{ steps.internal.outputs.python-changed }}" >> $GITHUB_OUTPUT
            echo "workflows-changed=${{ steps.internal.outputs.workflows-changed }}" >> $GITHUB_OUTPUT
            echo "any-code-changed=${{ steps.internal.outputs.any-code-changed }}" >> $GITHUB_OUTPUT
          fi

      - name: üìä Debug Publishing Conditions
        run: |
          echo "## üîç Publishing Condition Debug" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Event name**: ${{ github.event_name }}" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Branch reference**: ${{ github.ref }}" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Branch name**: ${{ github.ref_name }}" >> "$GITHUB_STEP_SUMMARY"
          echo "- **External call**: ${{ steps.context.outputs.external-call }}" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Docs changed**: ${{ steps.final.outputs.docs-changed }}" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Full scan input**: ${{ inputs.full_scan }}" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "### Publishing will run if:" >> "$GITHUB_STEP_SUMMARY"
          echo "1. Event is 'workflow_call/workflow_dispatch' ‚úÖ: ${{ github.event_name == 'workflow_call' ||
            github.event_name == 'workflow_dispatch' }}" >> "$GITHUB_STEP_SUMMARY"
          echo "2. External call OR branch is main/release/hotfix ‚úÖ: ${{ steps.context.outputs.external-call == 'true' ||
            github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/heads/release/') ||
            startsWith(github.ref, 'refs/heads/hotfix/') }}" >> "$GITHUB_STEP_SUMMARY"
          echo "3. Docs changed OR full_scan OR external call ‚úÖ: ${{ steps.final.outputs.docs-changed == 'true' ||
            inputs.full_scan == true || steps.context.outputs.external-call == 'true' }}" >> "$GITHUB_STEP_SUMMARY"

  super-linter:
    name: üîç Super Linter
    runs-on: ubuntu-latest
    needs: detect-changes
    if: needs.detect-changes.outputs.any-code-changed == 'true' || inputs.full_scan == true

    permissions:
      contents: read   # Base permission for reading code
      packages: read
      statuses: write
      security-events: write

    steps:
      - name: üì• Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: ${{ inputs.branch_name || github.ref }}

      - name: üîç Detect Repository Context for Linting
        id: context
        run: |
          echo "Current repository: ${{ github.repository }}"
          echo "Event name: ${{ github.event_name }}"

          # Check if we're in the redesigned-guacamole repo or being called remotely
          if [[ "${{ github.repository }}" == *"redesigned-guacamole" ]]; then
            echo "üè† Running within redesigned-guacamole repository"
            echo "same-repo=true" >> $GITHUB_OUTPUT
          else
            echo "üåê Being called from external repository: ${{ github.repository }}"
            echo "same-repo=false" >> $GITHUB_OUTPUT
          fi

      - name: üì• Checkout Redesigned-Guacamole (Lint Configs)
        if: steps.context.outputs.same-repo != 'true'
        uses: actions/checkout@v4
        with:
          repository: ${{ github.repository_owner }}/redesigned-guacamole
          ref: develop
          fetch-depth: 1
          path: .lint-configs

      - name: üìã Copy Lint Configuration Files
        if: steps.context.outputs.same-repo != 'true'
        run: |
          echo "üìã Copying lint configuration files from redesigned-guacamole..."

          # List of lint configuration files to copy
          LINT_FILES=(
            ".ansible-lint"
            ".yamllint"
            ".markdownlint.json"
            ".markdownlint.yml"
            ".flake8"
            ".pylintrc"
            ".eslintrc.json"
            ".eslintrc.yml"
            ".eslintrc.yaml"
            ".eslintrc.js"
            ".jscpd.json"
            ".jsonlintrc"
            ".shellcheckrc"
            ".shfmt"
            ".editorconfig"
            ".editorconfig-checker.json"
            ".prettierrc.json"
            ".prettierrc.yml"
            ".textlintrc"
            ".gitleaks.toml"
            ".pre-commit-config.yaml"
            "pyproject.toml"
          )

          copied_count=0
          for config_file in "${LINT_FILES[@]}"; do
            if [ -f ".lint-configs/$config_file" ]; then
              # Only copy if the file doesn't already exist in the calling repo
              if [ ! -f "$config_file" ]; then
                cp ".lint-configs/$config_file" "$config_file"
                echo "  ‚úÖ Copied $config_file from redesigned-guacamole"
                copied_count=$((copied_count + 1))
              else
                echo "  ‚ÑπÔ∏è  Skipped $config_file (already exists in calling repository)"
              fi
            fi
          done

          # Copy custom_rules directory if it exists (for Ansible custom rules)
          if [ -d ".lint-configs/custom_rules" ]; then
            if [ ! -d "custom_rules" ]; then
              cp -r ".lint-configs/custom_rules" "custom_rules"
              echo "  ‚úÖ Copied custom_rules/ directory from redesigned-guacamole"
              copied_count=$((copied_count + 1))
            else
              echo "  ‚ÑπÔ∏è  Skipped custom_rules/ (already exists in calling repository)"
            fi
          fi

          # Copy Super Linter configuration if it exists
          if [ -f ".lint-configs/.github/super-linter.env" ]; then
            mkdir -p .github
            if [ ! -f ".github/super-linter.env" ]; then
              cp ".lint-configs/.github/super-linter.env" ".github/super-linter.env"
              echo "  ‚úÖ Copied .github/super-linter.env from redesigned-guacamole"
              copied_count=$((copied_count + 1))
            else
              echo "  ‚ÑπÔ∏è  Skipped .github/super-linter.env (already exists in calling repository)"
            fi
          fi

          echo "üìä Summary: Copied $copied_count lint configuration files"
          echo "üéØ Lint configuration setup complete for external repository"

          # Clean up the temporary checkout
          rm -rf .lint-configs

      - name: üìã Verify Available Lint Configurations
        run: |
          echo "üìã Available lint configuration files in current workspace:"

          # List all available lint config files
          LINT_FILES=(
            ".ansible-lint"
            ".yamllint"
            ".markdownlint.json"
            ".markdownlint.yml"
            ".flake8"
            ".pylintrc"
            ".eslintrc.json"
            ".eslintrc.yml"
            ".eslintrc.yaml"
            ".eslintrc.js"
            ".jscpd.json"
            ".jsonlintrc"
            ".shellcheckrc"
            ".shfmt"
            ".editorconfig"
            ".github/super-linter.env"
            "pyproject.toml"
          )

          found_count=0
          for config_file in "${LINT_FILES[@]}"; do
            if [ -f "$config_file" ]; then
              echo "  ‚úÖ $config_file ($(wc -l < "$config_file" 2>/dev/null || echo "binary") lines)"
              found_count=$((found_count + 1))
            fi
          done

          echo "üìä Total lint configuration files available: $found_count"

          if [ "$found_count" -eq 0 ]; then
            echo "‚ö†Ô∏è  No lint configuration files found - Super Linter will use defaults"
          else
            echo "üéØ Super Linter will use the available configuration files for enhanced linting"
          fi

      - name: ÔøΩ Check Write Permissions
        id: check-permissions
        run: |
          # Try to create a test file to check write permissions
          if echo "test" > .permission-test 2>/dev/null && rm .permission-test 2>/dev/null; then
            echo "has_write_access=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Write permissions available - auto-fixes can be committed"
          else
            echo "has_write_access=false" >> $GITHUB_OUTPUT
            echo "‚ÑπÔ∏è  Read-only mode - auto-fixes will run but not be committed"
          fi

          # Also check if auto_fix input is enabled (default: true)
          if [ "${{ inputs.auto_fix }}" = "false" ]; then
            auto_fix_enabled="false"
            echo "üîß Auto-fix explicitly disabled (input: ${{ inputs.auto_fix }})"
          else
            auto_fix_enabled="true"
            echo "üîß Auto-fix enabled (input: ${{ inputs.auto_fix || 'default true' }})"
          fi
          echo "üêõ Setting auto_fix_enabled=$auto_fix_enabled"
          echo "auto_fix_enabled=$auto_fix_enabled" >> $GITHUB_OUTPUT

      - name: ÔøΩüîß Configure Environment
        id: config
        run: |
          # Dynamic branch detection
          if [ "${{ github.event.repository.default_branch }}" != "" ]; then
            echo "default_branch=${{ github.event.repository.default_branch }}" >> $GITHUB_OUTPUT
          else
            echo "default_branch=main" >> $GITHUB_OUTPUT
          fi

      - name: üîç Validate Critical Linter Configurations
        run: |
          echo "üîç Validating critical linter configurations..."

          # Validate .ansible-lint if present
          if [ -f ".ansible-lint" ]; then
            echo "‚úÖ .ansible-lint found - validating syntax..."
            if python3 -c "import yaml; yaml.safe_load(open('.ansible-lint'))" 2>/dev/null; then
              echo "  ‚úÖ .ansible-lint syntax is valid"
            else
              echo "  ‚ö†Ô∏è  .ansible-lint has syntax issues"
            fi
          fi

          # Validate .yamllint if present
          if [ -f ".yamllint" ]; then
            echo "‚úÖ .yamllint found - validating syntax..."
            if python3 -c "import yaml; yaml.safe_load(open('.yamllint'))" 2>/dev/null; then
              echo "  ‚úÖ .yamllint syntax is valid"
            else
              echo "  ‚ö†Ô∏è  .yamllint has syntax issues"
            fi
          fi

          # Validate markdownlint config if present
          if [ -f ".markdownlint.json" ]; then
            echo "‚úÖ .markdownlint.json found - validating syntax..."
            if python3 -c "import json; json.load(open('.markdownlint.json'))" 2>/dev/null; then
              echo "  ‚úÖ .markdownlint.json syntax is valid"
            else
              echo "  ‚ö†Ô∏è  .markdownlint.json has syntax issues"
            fi
          fi

          # Check for conflicting config files
          if [ -f ".markdownlint.json" ] && [ -f ".markdownlint.yml" ]; then
            echo "‚ö†Ô∏è  Both .markdownlint.json and .markdownlint.yml found - Super Linter will prefer .json"
          fi

          echo "üéØ Configuration validation complete"

      - name: üîç Detect Files for Validation
        id: detect-files
        run: |
          echo "üîç Detecting file types for intelligent validation..."

          # Check for different file types and set outputs (with proper quoting)
          yaml_files=$(find . -type f \( -name "*.yml" -o -name "*.yaml" \) ! -path "./.git/*" | wc -l)
          ansible_files=$(find . -type f \( -path "./playbooks/*" -o -path "./roles/*" \
            -o -name "site.yml" -o -name "playbook*.yml" -o -name "playbook*.yaml" \) | wc -l)
          python_files=$(find . -type f -name "*.py" ! -path "./.git/*" ! -path "./.venv/*" ! -path "./venv/*" ! -path "./.mypy_cache/*" ! -path "./__pycache__/*" | wc -l)
          markdown_files=$(find . -type f -name "*.md" ! -path "./.git/*" | wc -l)
          shell_files=$(find . -type f -name "*.sh" ! -path "./.git/*" | wc -l)
          json_files=$(find . -type f -name "*.json" ! -path "./.git/*" ! -path "./node_modules/*" ! -path "./.mypy_cache/*" ! -path "./.pytest_cache/*" ! -path "./.venv/*" ! -path "./venv/*" ! -path "./.tox/*" ! -path "./__pycache__/*" | wc -l)
          github_actions_files=$(find .github/workflows -type f \( -name "*.yml" -o -name "*.yaml" \) 2>/dev/null |
            wc -l)

          # Set outputs
          echo "yaml_files=$yaml_files" >> $GITHUB_OUTPUT
          echo "ansible_files=$ansible_files" >> $GITHUB_OUTPUT
          echo "python_files=$python_files" >> $GITHUB_OUTPUT
          echo "markdown_files=$markdown_files" >> $GITHUB_OUTPUT
          echo "shell_files=$shell_files" >> $GITHUB_OUTPUT
          echo "json_files=$json_files" >> $GITHUB_OUTPUT
          echo "github_actions_files=$github_actions_files" >> $GITHUB_OUTPUT

          # Output summary
          echo "üéØ File detection summary:"
          echo "  - YAML files found: $yaml_files"
          echo "  - Ansible files found: $ansible_files"
          echo "  - Python files found: $python_files"
          echo "  - Markdown files found: $markdown_files"
          echo "  - Shell files found: $shell_files"
          echo "  - JSON files found: $json_files"
          echo "  - GitHub Actions files found: $github_actions_files"
          echo ""
          echo "üìã Linters that will be enabled based on file detection:"
          [ "$yaml_files" -gt 0 ] && echo "  ‚úÖ YAML/yamllint"
          [ "$ansible_files" -gt 0 ] && echo "  ‚úÖ Ansible/ansible-lint"
          [ "$markdown_files" -gt 0 ] && echo "  ‚úÖ Markdown/markdownlint"
          [ "$python_files" -gt 0 ] && echo "  ‚úÖ Python/flake8"
          [ "$shell_files" -gt 0 ] && echo "  ‚úÖ Shell/ShellCheck"
          [ "$json_files" -gt 0 ] && echo "  ‚úÖ JSON/jsonlint"
          [ "$github_actions_files" -gt 0 ] && echo "  ‚úÖ GitHub Actions/actionlint"

      - name: üîç Verify Configuration Files
        run: |
          echo "üîç Verifying Super Linter configuration..."

          # Check for Super Linter config (may not exist in remote repositories)
          if [ -f ".github/super-linter.env" ]; then
            echo "‚úÖ Found .github/super-linter.env configuration file:"
            cat .github/super-linter.env
          else
            echo "‚ÑπÔ∏è No .github/super-linter.env found - using Super Linter defaults"
            echo "üìù Super Linter will use built-in defaults and detect configuration files automatically"
          fi

          echo ""
          echo "üîç Verifying yamllint configuration:"
          if [ -f ".yamllint" ]; then
            echo "‚úÖ .yamllint file exists"
            echo "üìÑ .yamllint line-length configuration:"
            grep -A 10 "line-length:" .yamllint || echo "‚ùå line-length not found in .yamllint"
          else
            echo "‚ÑπÔ∏è .yamllint file not found - Super Linter will use defaults"
          fi

      - name: üîç Run Super Linter
        id: super-linter
        uses: super-linter/super-linter@v5
        env:
          # Use static configuration file if it exists (only in redesigned-guacamole repo)
          ENV_FILE: ${{ hashFiles('.github/super-linter.env') != '' && '.github/super-linter.env' || '' }}

          # GitHub token (required for API access)
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

          # Branch and validation settings
          DEFAULT_BRANCH: ${{ steps.config.outputs.default_branch }}
          VALIDATE_ALL_CODEBASE: ${{ inputs.full_scan == true }}

      - name: ÔøΩ Enhanced Super Linter Analysis
        if: always()
        id: enhanced-analysis
        run: |
          echo "::group::üîç Enhanced Super Linter Analysis"

          # Run comprehensive analysis that includes autofix detection and shell script analysis
          echo "ÔøΩ Running enhanced code quality analysis with shell script tracking..."

          # Enhanced analysis including shell script issues
          python3 << 'EOF'
          import os
          import json
          import glob
          import sys
          import subprocess
          from pathlib import Path

          def analyze_shell_scripts():
              """Analyze shell scripts specifically for ShellCheck issues"""
              shell_files = []
              # Find shell script files
              for pattern in ['**/*.sh', '**/*.bash']:
                  shell_files.extend(glob.glob(pattern, recursive=True))

              # Also check workflow files for embedded shell scripts
              workflow_files = glob.glob('.github/workflows/*.yml', recursive=True)

              print(f"üêö Found {len(shell_files)} shell script files for analysis")
              print(f"üìã Found {len(workflow_files)} workflow files to check for shell scripts")

              shell_issues = 0
              shell_fixes = 0

              # Analyze each shell file with ShellCheck if available
              for shell_file in shell_files:
                  if os.path.exists(shell_file):
                      try:
                          # Use shellcheck to analyze the file
                          result = subprocess.run(['shellcheck', '-f', 'json', shell_file],
                                                capture_output=True, text=True, timeout=30)
                          if result.stdout:
                              issues = json.loads(result.stdout)
                              file_issues = len(issues)
                              shell_issues += file_issues
                              # Count auto-fixable issues
                              auto_fixable = sum(1 for issue in issues
                                               if issue.get('code') in [
                                                   2086,  # Unquoted variables
                                                   2048,  # Invalid number
                                                   2006,  # Use $(...) instead of backticks
                                                   2035,  # Missing quotes
                                                   2129,  # Multiple redirections
                                               ])
                              shell_fixes += auto_fixable
                              if file_issues > 0:
                                  print(f"  üêö {shell_file}: {file_issues} issues, {auto_fixable} auto-fixable")
                      except (json.JSONDecodeError, subprocess.SubprocessError, subprocess.TimeoutExpired, FileNotFoundError) as e:
                          print(f"  ‚ÑπÔ∏è  ShellCheck analysis skipped for {shell_file}: {type(e).__name__}")

              return shell_issues, shell_fixes

          def get_super_linter_results():
              """Parse Super Linter output for comprehensive analysis"""
              results = {
                  'yaml_issues': 0, 'yaml_fixes': 0,
                  'ansible_issues': 0, 'ansible_fixes': 0,
                  'python_issues': 0, 'python_fixes': 0,
                  'shell_issues': 0, 'shell_fixes': 0,
                  'markdown_issues': 0, 'markdown_fixes': 0,
                  'json_issues': 0, 'json_fixes': 0,
                  'total_issues': 0, 'total_fixes': 0
              }

              # Analyze shell scripts specifically
              shell_issues, shell_fixes = analyze_shell_scripts()
              results['shell_issues'] = shell_issues
              results['shell_fixes'] = shell_fixes

              # Check for Super Linter output files
              report_dirs = ["super-linter.report", "./", "reports"]
              for report_dir in report_dirs:
                  if os.path.exists(report_dir):
                      print(f"üìÅ Checking directory: {report_dir}")
                      for report_file in glob.glob(f"{report_dir}/**/*.log", recursive=True):
                          print(f"  üìÑ Found report: {os.path.basename(report_file)}")
                      for report_file in glob.glob(f"{report_dir}/**/*.tap", recursive=True):
                          print(f"  üìä Found TAP report: {os.path.basename(report_file)}")

              # Calculate totals
              results['total_issues'] = sum(v for k, v in results.items() if k.endswith('_issues'))
              results['total_fixes'] = sum(v for k, v in results.items() if k.endswith('_fixes'))

              # Calculate health score
              max_possible_score = 100
              if results['total_issues'] > 0:
                  # Deduct points based on issues found
                  deduction = min(results['total_issues'] * 2, 80)  # Max 80 point deduction
                  health_score = max(max_possible_score - deduction, 20)  # Min score of 20
              else:
                  health_score = max_possible_score

              results['overall_health_score'] = health_score

              return results

          # Main analysis
          print("üîç Starting enhanced Super Linter analysis with shell script tracking...")
          results = get_super_linter_results()

          # Output results for GitHub Actions
          print(f"üìä Analysis Results:")
          print(f"  üêö Shell issues: {results['shell_issues']} (fixes: {results['shell_fixes']})")
          print(f"  üìÑ YAML issues: {results['yaml_issues']} (fixes: {results['yaml_fixes']})")
          print(f"  üé≠ Ansible issues: {results['ansible_issues']} (fixes: {results['ansible_fixes']})")
          print(f"  üêç Python issues: {results['python_issues']} (fixes: {results['python_fixes']})")
          print(f"  üìù Markdown issues: {results['markdown_issues']} (fixes: {results['markdown_fixes']})")
          print(f"  üìã JSON issues: {results['json_issues']} (fixes: {results['json_fixes']})")
          print(f"  üè• Overall health score: {results['overall_health_score']}/100")

          # Set GitHub Actions outputs
          autofix_needed = "true" if results['total_fixes'] > 0 else "false"

          # Write outputs to GitHub Actions
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"autofix_needed={autofix_needed}\n")
              f.write(f"total_fixes={results['total_fixes']}\n")
              f.write(f"total_issues={results['total_issues']}\n")
              f.write(f"shell_issues={results['shell_issues']}\n")
              f.write(f"shell_fixes={results['shell_fixes']}\n")
              f.write(f"yaml_issues={results['yaml_issues']}\n")
              f.write(f"yaml_fixes={results['yaml_fixes']}\n")
              f.write(f"ansible_issues={results['ansible_issues']}\n")
              f.write(f"ansible_fixes={results['ansible_fixes']}\n")
              f.write(f"python_issues={results['python_issues']}\n")
              f.write(f"python_fixes={results['python_fixes']}\n")
              f.write(f"markdown_issues={results['markdown_issues']}\n")
              f.write(f"markdown_fixes={results['markdown_fixes']}\n")
              f.write(f"json_issues={results['json_issues']}\n")
              f.write(f"json_fixes={results['json_fixes']}\n")
              f.write(f"overall_health_score={results['overall_health_score']}\n")

          print("‚úÖ Enhanced analysis with shell script tracking completed")
          EOF

          echo "‚úÖ Enhanced analysis completed"
          echo "::endgroup::"
        working-directory: ${{ github.workspace }}

      - name: üìä Enhanced Analysis Summary
        if: always()
        run: |
          {
            echo "## üîç Enhanced Super Linter Analysis Results"
            echo ""
            echo "### üìã Code Quality Analysis"
            echo "- **üêö Shell script issues**: ${{ steps.enhanced-analysis.outputs.shell_issues || '0' }} (auto-fixable: ${{ steps.enhanced-analysis.outputs.shell_fixes || '0' }})"
            echo "- **üìÑ YAML issues**: ${{ steps.enhanced-analysis.outputs.yaml_issues || '0' }} (auto-fixable: ${{ steps.enhanced-analysis.outputs.yaml_fixes || '0' }})"
            echo "- **üé≠ Ansible issues**: ${{ steps.enhanced-analysis.outputs.ansible_issues || '0' }} (auto-fixable: ${{ steps.enhanced-analysis.outputs.ansible_fixes || '0' }})"
            echo "- **üêç Python issues**: ${{ steps.enhanced-analysis.outputs.python_issues || '0' }} (auto-fixable: ${{ steps.enhanced-analysis.outputs.python_fixes || '0' }})"
            echo "- **üìù Markdown issues**: ${{ steps.enhanced-analysis.outputs.markdown_issues || '0' }} (auto-fixable: ${{ steps.enhanced-analysis.outputs.markdown_fixes || '0' }})"
            echo "- **üìã JSON issues**: ${{ steps.enhanced-analysis.outputs.json_issues || '0' }} (auto-fixable: ${{ steps.enhanced-analysis.outputs.json_fixes || '0' }})"
            echo ""
            echo "### üè• Overall Health Score"
            echo "**Score**: ${{ steps.enhanced-analysis.outputs.overall_health_score || 'N/A' }}/100"
            echo ""
            echo "### ü§ñ Auto-fix Status"
            if [ "${{ steps.enhanced-analysis.outputs.autofix_needed }}" = "true" ]; then
              echo "‚úÖ **Auto-fixes available**: ${{ steps.enhanced-analysis.outputs.total_fixes }} potential fixes detected"
              if [ "${{ steps.check-permissions.outputs.has_write_access }}" = "true" ]; then
                echo "üìù **Commit mode**: Auto-fixes will be applied and committed"
              else
                echo "üëÄ **Read-only mode**: Auto-fixes will be applied but not committed"
              fi
            else
              echo "üéâ **No auto-fixes needed**: Code quality is excellent!"
            fi
            echo ""
            echo "### üîç Analysis Coverage"
            echo "- Shell script analysis with ShellCheck integration"
            echo "- Multi-language linting with Super Linter"
            echo "- Auto-fixable issue detection and classification"
            echo "- Health score calculation based on issue severity"
          } >> "$GITHUB_STEP_SUMMARY"

      - name: üêõ Debug Auto-fix Conditions
        if: always()
        run: |
          echo "üêõ Debugging auto-fix step conditions:"
          echo "  - auto_fix input: '${{ inputs.auto_fix }}'"
          echo "  - auto_fix_enabled: '${{ steps.check-permissions.outputs.auto_fix_enabled }}'"
          echo "  - has_write_access: '${{ steps.check-permissions.outputs.has_write_access }}'"
          echo "  - autofix_needed: '${{ steps.enhanced-analysis.outputs.autofix_needed }}'"
          echo "  - Condition for auto-fix: always() && steps.check-permissions.outputs.auto_fix_enabled == 'true'"

      - name: ü§ñ Apply Auto-fixes
        if: steps.check-permissions.outputs.auto_fix_enabled == 'true'
        id: apply-fixes
        run: |
          echo "ü§ñ Applying auto-fixes to detected issues..."
          echo "üîç Auto-fix mode: ${{ inputs.auto_fix || 'true (default)' }}"
          echo "üîê Write access: ${{ steps.check-permissions.outputs.has_write_access }}"

          # Initialize counters
          total_fixes=0
          yaml_fixes=0
          python_fixes=0
          shell_fixes=0
          markdown_fixes=0
          json_fixes=0

          # Python auto-fixes with Black and isort
          echo "üêç Applying Python auto-fixes..."

          # Debug: List Python files found
          python_files_found=$(find . -name "*.py" -not -path "./.git/*" -not -path "./.venv/*" -not -path "./venv/*" -not -path "./node_modules/*" -not -path "./.mypy_cache/*" -not -path "./__pycache__/*")
          python_file_count=$(echo "$python_files_found" | grep -c "\.py$" || echo "0")
          echo "  üîç Found $python_file_count Python files to check"
          if [ "$python_file_count" -gt 0 ] && [ "$python_file_count" -le 5 ]; then
            echo "    Files: $python_files_found"
          fi
          if command -v black >/dev/null 2>&1; then
            if find . -name "*.py" -not -path "./.git/*" -not -path "./.venv/*" -not -path "./venv/*" -not -path "./node_modules/*" -not -path "./.mypy_cache/*" -not -path "./__pycache__/*" | xargs black --check --quiet 2>/dev/null; then
              echo "  ‚úÖ Python files already formatted correctly"
            else
              echo "  ÔøΩ Formatting Python files with Black..."
              python_black_count=$(find . -name "*.py" -not -path "./.git/*" -not -path "./.venv/*" -not -path "./venv/*" -not -path "./node_modules/*" -not -path "./.mypy_cache/*" -not -path "./__pycache__/*" | xargs black --diff --quiet 2>&1 | grep -c "would reformat" || echo "0")
              find . -name "*.py" -not -path "./.git/*" -not -path "./.venv/*" -not -path "./venv/*" -not -path "./node_modules/*" -not -path "./.mypy_cache/*" -not -path "./__pycache__/*" | xargs black
              python_fixes=$((python_fixes + python_black_count))
              total_fixes=$((total_fixes + python_black_count))
              echo "    Applied $python_black_count Black formatting fixes"
            fi
          fi

          if command -v isort >/dev/null 2>&1; then
            echo "  üîß Sorting Python imports with isort..."
            python_isort_count=$(find . -name "*.py" -not -path "./.git/*" -not -path "./.venv/*" -not -path "./venv/*" -not -path "./node_modules/*" -not -path "./.mypy_cache/*" -not -path "./__pycache__/*" | xargs isort --check-only --diff 2>&1 | grep -c "Fixing" || echo "0")
            find . -name "*.py" -not -path "./.venv/*" -not -path "./venv/*" -not -path "./node_modules/*" -not -path "./.mypy_cache/*" -not -path "./__pycache__/*" | xargs isort
            python_fixes=$((python_fixes + python_isort_count))
            total_fixes=$((total_fixes + python_isort_count))
            echo "    Applied $python_isort_count import sorting fixes"
          fi

          # YAML auto-fixes (basic formatting)
          echo "üìÑ Checking YAML files..."
          yaml_count=0
          for file in $(find . -name "*.yml" -o -name "*.yaml" | grep -v ".git"); do
            if yamllint "$file" >/dev/null 2>&1; then
              continue
            else
              # Simple YAML fixes: trailing whitespace
              if sed -i 's/[[:space:]]*$//' "$file"; then
                yaml_count=$((yaml_count + 1))
              fi
            fi
          done
          yaml_fixes=$yaml_count
          total_fixes=$((total_fixes + yaml_count))
          echo "  Applied $yaml_count YAML formatting fixes"

          # Shell script auto-fixes with shfmt
          echo "üêö Applying shell script auto-fixes..."
          if command -v shfmt >/dev/null 2>&1; then
            shell_count=0
            for file in $(find . -name "*.sh" | grep -v ".git"); do
              if shfmt -d "$file" >/dev/null 2>&1; then
                shfmt -w "$file"
                shell_count=$((shell_count + 1))
              fi
            done
            shell_fixes=$shell_count
            total_fixes=$((total_fixes + shell_count))
            echo "  Applied $shell_count shell script formatting fixes"
          fi

          # JSON auto-fixes
          echo "üìã Applying JSON auto-fixes..."
          json_count=0
          for file in $(find . -name "*.json" \
            -not -path "./.git/*" \
            -not -path "./node_modules/*" \
            -not -path "./.mypy_cache/*" \
            -not -path "./.pytest_cache/*" \
            -not -path "./.venv/*" \
            -not -path "./venv/*" \
            -not -path "./.tox/*" \
            -not -path "./__pycache__/*" \
            -not -name "package-lock.json" \
            -not -name "*.tmp"); do
            # Skip if file is not writable
            if [ ! -w "$file" ]; then
              continue
            fi
            if python3 -m json.tool "$file" > "${file}.tmp" 2>/dev/null; then
              if ! diff -q "$file" "${file}.tmp" >/dev/null 2>&1; then
                mv "${file}.tmp" "$file"
                json_count=$((json_count + 1))
              else
                rm -f "${file}.tmp"
              fi
            else
              rm -f "${file}.tmp"
            fi
          done
          json_fixes=$json_count
          total_fixes=$((total_fixes + json_count))
          echo "  Applied $json_count JSON formatting fixes"

          # Markdown auto-fixes
          echo "üìù Applying Markdown auto-fixes..."
          markdown_count=0

          # Method 1: Basic markdown fixes (trailing whitespace, line endings)
          for file in $(find . -name "*.md" | grep -v ".git" | grep -v "node_modules"); do
            original_size=$(wc -c < "$file" 2>/dev/null || echo "0")

            # Fix trailing whitespace
            sed -i 's/[[:space:]]*$//' "$file" 2>/dev/null || true

            # Ensure file ends with newline
            if [ -s "$file" ] && [ "$(tail -c1 "$file" 2>/dev/null)" != "" ]; then
              echo "" >> "$file"
            fi

            # Fix multiple consecutive empty lines (reduce to max 2)
            awk '/^$/{if(++n<=2)print;next}{n=0;print}' "$file" > "${file}.tmp" && mv "${file}.tmp" "$file" 2>/dev/null || true

            new_size=$(wc -c < "$file" 2>/dev/null || echo "0")
            if [ "$original_size" != "$new_size" ]; then
              markdown_count=$((markdown_count + 1))
            fi
          done

          # Method 2: Try markdownlint-cli2 auto-fix if available
          if command -v markdownlint-cli2 >/dev/null 2>&1; then
            echo "  üîß Running markdownlint-cli2 auto-fix..."
            find . -name "*.md" -not -path "./.git/*" -not -path "./node_modules/*" | \
              xargs markdownlint-cli2 --fix --config .markdownlint.json 2>/dev/null || true
          elif command -v markdownlint >/dev/null 2>&1; then
            echo "  üîß Running markdownlint auto-fix..."
            find . -name "*.md" -not -path "./.git/*" -not -path "./node_modules/*" | \
              xargs markdownlint --fix --config .markdownlint.json 2>/dev/null || true
          fi

          # Method 3: Try prettier for markdown if available
          if command -v prettier >/dev/null 2>&1; then
            echo "  üîß Running prettier for markdown..."
            find . -name "*.md" -not -path "./.git/*" -not -path "./node_modules/*" | \
              xargs prettier --write --parser markdown --prose-wrap preserve 2>/dev/null || true
          fi

          markdown_fixes=$markdown_count
          total_fixes=$((total_fixes + markdown_count))
          echo "  Applied $markdown_count markdown formatting fixes"

          # Set outputs for the commit step
          autofix_needed=$([ $total_fixes -gt 0 ] && echo true || echo false)
          echo "autofix_needed=$autofix_needed" >> $GITHUB_OUTPUT
          echo "total_fixes=$total_fixes" >> $GITHUB_OUTPUT
          echo "python_fixes=$python_fixes" >> $GITHUB_OUTPUT
          echo "yaml_fixes=$yaml_fixes" >> $GITHUB_OUTPUT
          echo "shell_fixes=$shell_fixes" >> $GITHUB_OUTPUT
          echo "json_fixes=$json_fixes" >> $GITHUB_OUTPUT
          echo "markdown_fixes=$markdown_fixes" >> $GITHUB_OUTPUT
          echo "ansible_fixes=0" >> $GITHUB_OUTPUT

          echo "üéØ Auto-fix summary: $total_fixes total fixes applied"
          echo "üìä Autofix status: $autofix_needed (total_fixes=$total_fixes)"

          # Always show what was checked, even if no fixes needed
          echo "üìã Auto-fix scan results:"
          echo "  - Python files checked: $(find . -name "*.py" -not -path "./.git/*" -not -path "./.venv/*" -not -path "./venv/*" -not -path "./node_modules/*" -not -path "./.mypy_cache/*" -not -path "./__pycache__/*" | wc -l)"
          echo "  - JSON files checked: $(find . -name "*.json" -not -path "./.git/*" -not -path "./node_modules/*" -not -path "./.mypy_cache/*" -not -path "./.pytest_cache/*" -not -path "./.venv/*" -not -path "./venv/*" -not -path "./.tox/*" -not -path "./__pycache__/*" -not -name "package-lock.json" -not -name "*.tmp" | wc -l)"

      - name: üíæ Commit Auto-fixes
        if: steps.apply-fixes.outputs.autofix_needed == 'true' && steps.check-permissions.outputs.has_write_access == 'true'
        run: |
          echo "üíæ Committing auto-fixes..."

          # Configure git (use GitHub Actions bot)
          git config --local user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"

          # Add all changes
          git add .

          # Check if there are actually changes to commit
          if git diff --staged --quiet; then
            echo "‚ÑπÔ∏è No changes to commit after autofix attempts"
            exit 0
          fi

          # Create detailed commit message
          commit_msg="ü§ñ Auto-fix: Applied ${{ steps.apply-fixes.outputs.total_fixes }} linting fixes

          Auto-fixes applied by formatters:
          - Shell fixes: ${{ steps.apply-fixes.outputs.shell_fixes }}
          - YAML fixes: ${{ steps.apply-fixes.outputs.yaml_fixes }}
          - Python fixes: ${{ steps.apply-fixes.outputs.python_fixes }}
          - Markdown fixes: ${{ steps.apply-fixes.outputs.markdown_fixes }}
          - JSON fixes: ${{ steps.apply-fixes.outputs.json_fixes }}

          Automated by: ${{ github.workflow }} #${{ github.run_number }}
          Event: ${{ github.event_name }}
          Health Score: ${{ steps.enhanced-analysis.outputs.overall_health_score }}/100"

          # Commit changes
          git commit -m "$commit_msg"

          # Push changes back to the branch
          git push origin ${{ github.ref_name }}

          echo "‚úÖ Auto-fixes committed and pushed!"

      - name: ‚ÑπÔ∏è Auto-fix Applied (Read-only Mode)
        if: steps.apply-fixes.outputs.autofix_needed == 'true' && steps.check-permissions.outputs.has_write_access != 'true'
        run: |
          echo "üîß Auto-fixes were applied but could not be committed (read-only mode)"
          echo "üìã To enable auto-fix commits in remote workflows:"
          echo "  1. Grant 'contents: write' permission in the calling workflow"
          echo "  2. Or run the workflow directly on this repository"
          echo ""
          echo "üìä Fixes that would have been applied:"
          echo "  - Python fixes: ${{ steps.apply-fixes.outputs.python_fixes || '0' }}"
          echo "  - JSON fixes: ${{ steps.apply-fixes.outputs.json_fixes || '0' }}"
          echo "  - YAML fixes: ${{ steps.apply-fixes.outputs.yaml_fixes || '0' }}"
          echo "  - Shell fixes: ${{ steps.apply-fixes.outputs.shell_fixes || '0' }}"
          echo "  - Markdown fixes: ${{ steps.apply-fixes.outputs.markdown_fixes || '0' }}"
          echo "  - Total fixes: ${{ steps.apply-fixes.outputs.total_fixes || '0' }}"

      - name: ‚ö†Ô∏è Auto-fix Status
        if: steps.enhanced-analysis.outputs.autofix_needed == 'true'
        run: |
          echo "üîç Auto-fixes applied successfully!"
          echo "ÔøΩ Summary of fixes:"
          echo "  - Shell fixes: ${{ steps.enhanced-analysis.outputs.shell_fixes }}"
          echo "  - YAML fixes: ${{ steps.enhanced-analysis.outputs.yaml_fixes }}"
          echo "  - Ansible fixes: ${{ steps.enhanced-analysis.outputs.ansible_fixes }}"
          echo "  - Python fixes: ${{ steps.enhanced-analysis.outputs.python_fixes }}"
          echo "  - Markdown fixes: ${{ steps.enhanced-analysis.outputs.markdown_fixes }}"
          echo "  - JSON fixes: ${{ steps.enhanced-analysis.outputs.json_fixes }}"
          echo ""
          echo "‚úÖ Changes have been committed and pushed back to the branch"

      - name: üì§ Upload Super Linter Logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: super-linter-logs-${{ github.run_id }}
          path: |
            super-linter.log
            github-super-linter.log
            .github/super-linter.env
          retention-days: 30
          if-no-files-found: ignore

      - name: üì§ Upload Static Configuration (Debug)
        if: always() && hashFiles('.github/super-linter.env') != ''
        uses: actions/upload-artifact@v4
        with:
          name: super-linter-config-${{ github.run_id }}
          path: .github/super-linter.env
          retention-days: 7
          if-no-files-found: warn

  # Enhanced security scanning - only runs when code changes
  security:
    name: üõ°Ô∏è Security Scan
    runs-on: ubuntu-latest
    needs: detect-changes
    if: needs.detect-changes.outputs.any-code-changed == 'true' || inputs.full_scan == true
    permissions:
      actions: read
      contents: read
      security-events: write
    steps:
      - name: üì• Checkout Code
        uses: actions/checkout@v4
        with:
          ref: ${{ inputs.branch_name || github.ref }}

      - name: üîç Run DevSkim Scanner
        uses: microsoft/DevSkim-Action@v1

      - name: üì§ Upload DevSkim SARIF
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: devskim-results.sarif

      - name: ‚ò¢Ô∏è Run Trivy Vulnerability Scanner
        uses: aquasecurity/trivy-action@0.32.0
        with:
          scan-type: 'fs'
          ignore-unfixed: true
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH'

      - name: üì§ Upload Trivy Scan Results
        uses: actions/upload-artifact@v4
        with:
          name: trivy-results
          path: trivy-results.sarif

      - name: üîê Advanced Secret Detection
        run: |
          echo "üîç Running security validation..."

          # Check for potential secrets (excluding false positives)
          echo "Checking for potential hardcoded secrets..."
          if grep -rE "(password|secret|api_key|auth_token|private_key):\s*['\"]?[A-Za-z0-9+/=]{10,}" . \
             --include="*.yml" --include="*.yaml" --include="*.py" --include="*.sh" \
             --exclude-dir=.git --exclude-dir=.github \
             --exclude="*example*" --exclude="*template*" \
             | grep -v "YOUR_.*_HERE\|test:test\|example\|template\|#.*token\|#.*secret\|README"; then
            echo "‚ö†Ô∏è Potential secrets found - please review"
            exit 1
          else
            echo "‚úÖ No obvious secrets detected"
          fi

      - name: üîí File Permissions Check
        run: |
          echo "üîí Checking file permissions..."

          # Check for world-writable files
          if find . -name "*.yml" -o -name "*.yaml" -o -name "*.py" -o -name "*.sh" |
            xargs ls -la | grep "^-.......rw"; then
            echo "‚ùå World-writable files found"
            find . -name "*.yml" -o -name "*.yaml" -o -name "*.py" -o -name "*.sh" | xargs ls -la | grep "^-.......rw"
            exit 1
          else
            echo "‚úÖ File permissions look secure"
          fi

      - name: üõ°Ô∏è Security Summary
        if: always()
        run: |
          {
            echo "## üõ°Ô∏è Security Validation Summary"
            echo "‚úÖ **DevSkim scan completed**"
            echo "‚úÖ **Secret detection completed**"
            echo "‚úÖ **File permissions checked**"
          } >> "$GITHUB_STEP_SUMMARY"

  # Publish job - runs on main/release/hotfix branches when docs change, regardless of linting results
  publish:
    name: üöÄ Publish to Confluence
    runs-on: ubuntu-latest

    permissions:
      contents: read
      actions: read

    steps:
      - name: üîç Detect Repository Context
        id: context
        run: |
          echo "Current repository: ${{ github.repository }}"
          echo "Event name: ${{ github.event_name }}"

          # Check if we're in the redesigned-guacamole repo or being called remotely
          if [[ "${{ github.repository }}" == *"redesigned-guacamole" ]]; then
            echo "üè† Running within redesigned-guacamole repository"
            echo "same-repo=true" >> $GITHUB_OUTPUT
          else
            echo "üåê Being called from external repository: ${{ github.repository }}"
            echo "same-repo=false" >> $GITHUB_OUTPUT
          fi

          echo "üêõ Debug: same-repo detection result: $([ "${{ github.repository }}" == *"redesigned-guacamole" ] && echo "true" || echo "false")"

      - name: üì• Checkout Redesigned-Guacamole (Scripts)
        uses: actions/checkout@v4
        with:
          repository: ${{ github.repository_owner }}/redesigned-guacamole
          ref: main
          fetch-depth: 0
          path: redesigned-guacamole

      - name: ÔøΩ Debug After Redesigned-Guacamole Checkout
        run: |
          echo "üêõ After redesigned-guacamole checkout:"
          echo "  - Working directory: $(pwd)"
          echo "  - Contents: $(ls -la)"
          echo "  - Redesigned-guacamole exists: $([ -d redesigned-guacamole ] && echo 'YES' || echo 'NO')"
          if [ -d redesigned-guacamole ]; then
            echo "  - Redesigned-guacamole contents: $(ls -la redesigned-guacamole/)"
            echo "  - Scripts directory exists: $([ -d redesigned-guacamole/scripts ] && echo 'YES' || echo 'NO')"
            if [ -d redesigned-guacamole/scripts ]; then
              echo "  - Scripts directory contents: $(ls -la redesigned-guacamole/scripts/)"
              echo "  - confluence_publisher.py exists: $([ -f redesigned-guacamole/scripts/confluence_publisher.py ] && echo 'YES' || echo 'NO')"
            fi
          fi

      - name: ÔøΩüì• Checkout Calling Repository (Content)
        if: steps.context.outputs.same-repo != 'true'
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          path: calling-repo

      - name: üêõ Debug After Calling Repository Checkout
        if: steps.context.outputs.same-repo != 'true'
        run: |
          echo "üêõ After calling repository checkout:"
          echo "  - Working directory: $(pwd)"
          echo "  - Contents: $(ls -la)"
          echo "  - Calling-repo exists: $([ -d calling-repo ] && echo 'YES' || echo 'NO')"
          if [ -d calling-repo ]; then
            echo "  - Calling-repo contents: $(ls -la calling-repo/)"
          fi

      - name: üîó Create Content Symlink (Same Repository)
        if: steps.context.outputs.same-repo == 'true'
        run: |
          echo "üêõ Creating symlink for same repository scenario"
          # When running in the same repo, create a symlink to avoid duplication
          ln -s redesigned-guacamole calling-repo
          echo "‚úÖ Created symlink: calling-repo -> redesigned-guacamole"
          echo "üêõ Symlink verification: $(ls -la calling-repo/)"

      - name: ÔøΩÔ∏è Prevent Python Setup Auto-Detection
        run: |
          echo "üõ°Ô∏è Temporarily moving any requirements.txt files to prevent auto-detection..."
          # Move any requirements.txt or pyproject.toml in current dir to prevent Python setup from finding them
          if [ -f requirements.txt ]; then
            mv requirements.txt requirements.txt.tmp
            echo "üîÑ Moved requirements.txt to requirements.txt.tmp"
          fi
          if [ -f pyproject.toml ]; then
            mv pyproject.toml pyproject.toml.tmp
            echo "üîÑ Moved pyproject.toml to pyproject.toml.tmp"
          fi

      - name: ÔøΩüêç Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          # No cache to avoid dependency issues when called remotely

      - name: üîÑ Restore Project Files
        run: |
          echo "üîÑ Restoring any temporarily moved project files..."
          if [ -f requirements.txt.tmp ]; then
            mv requirements.txt.tmp requirements.txt
            echo "‚úÖ Restored requirements.txt"
          fi
          if [ -f pyproject.toml.tmp ]; then
            mv pyproject.toml.tmp pyproject.toml
            echo "‚úÖ Restored pyproject.toml"
          fi

      - name: üì¶ Install Dependencies
        run: |
          echo "üêõ Debug: Current working directory: $(pwd)"
          echo "üêõ Debug: Directory contents: $(ls -la)"
          echo "üêõ Debug: Redesigned-guacamole directory: $(ls -la redesigned-guacamole/ 2>/dev/null || echo 'Not found')"
          echo "üêõ Debug: Calling-repo directory: $(ls -la calling-repo/ 2>/dev/null || echo 'Not found')"

          python -m pip install --upgrade pip

          echo "üì¶ Installing dependencies for Confluence publishing..."

          # Try to install from redesigned-guacamole requirements first
          if [ -f redesigned-guacamole/requirements.txt ]; then
            echo "‚úÖ Found redesigned-guacamole/requirements.txt, installing..."
            pip install -r redesigned-guacamole/requirements.txt
          else
            echo "‚ö†Ô∏è No redesigned-guacamole/requirements.txt found"
            echo "üêõ Debug: Checking for requirements.txt in other locations:"
            find . -name "requirements.txt" -type f 2>/dev/null || echo "No requirements.txt found anywhere"
          fi

          # Always ensure core publishing dependencies are available
          echo "üîß Ensuring core Confluence publishing dependencies..."
          pip install jinja2 pyyaml requests markdown beautifulsoup4

          echo "‚úÖ Dependency installation complete"

      - name: üîç Validate Configuration
        run: |
          echo "üîç Validating Confluence configuration..."
          echo "üêõ Debug: Checking secret availability..."
          echo "  - CONFLUENCE_URL available: ${{ secrets.CONFLUENCE_URL != '' }}"
          echo "  - CONFLUENCE_USER available: ${{ secrets.CONFLUENCE_USER != '' }}"
          echo "  - CONFLUENCE_API_TOKEN available: ${{ secrets.CONFLUENCE_API_TOKEN != '' }}"
          echo "  - CONFLUENCE_URL length: ${#CONFLUENCE_URL}"
          echo "  - CONFLUENCE_USER length: ${#CONFLUENCE_USER}"
          echo "  - CONFLUENCE_TOKEN length: ${#CONFLUENCE_TOKEN}"

          # Auto-enable dry-run if secrets are missing
          if [[ -z "${{ secrets.CONFLUENCE_URL }}" ]] || [[ -z "${{ secrets.CONFLUENCE_USER }}" ]] || [[ -z "${{ secrets.CONFLUENCE_API_TOKEN }}" ]]; then
            echo "‚ö†Ô∏è Some Confluence secrets are missing - automatically enabling DRY RUN mode"
            echo "üìã To enable live publishing, add these secrets to your repository:"
            echo "   - CONFLUENCE_URL: Your Confluence instance URL"
            echo "   - CONFLUENCE_USER: Your Confluence username/email"
            echo "   - CONFLUENCE_API_TOKEN: Your Confluence API token"
            echo ""
            echo "üìù Example calling workflow:"
            echo "jobs:"
            echo "  publish:"
            echo "    uses: your-org/redesigned-guacamole/.github/workflows/ci-optimized.yml@main"
            echo "    secrets:"
            echo "      CONFLUENCE_URL: \${{ secrets.CONFLUENCE_URL }}"
            echo "      CONFLUENCE_USER: \${{ secrets.CONFLUENCE_USER }}"
            echo "      CONFLUENCE_API_TOKEN: \${{ secrets.CONFLUENCE_API_TOKEN }}"
            echo ""
            echo "üß™ Proceeding in DRY RUN mode..."
            forced_dry_run="true"
          else
            forced_dry_run="false"
          fi

          # Check if we're in dry-run mode (either explicitly or forced due to missing secrets)
          effective_dry_run="${{ inputs.dry_run }}"
          if [[ "$forced_dry_run" == "true" ]]; then
            effective_dry_run="true"
          fi

          if [[ "$effective_dry_run" == "true" ]]; then
            echo "üß™ Running in DRY RUN mode - secrets validation relaxed"
            echo "üß™ Dry run mode: proceeding with available configuration"
          else
            echo "üöÄ Running in LIVE mode - all secrets required"

            # Check if required secrets are provided for live run
            if [[ -z "${{ secrets.CONFLUENCE_URL }}" ]]; then
              echo "‚ùå CONFLUENCE_URL secret is required for live publishing"
              echo "üêõ Debug: CONFLUENCE_URL is empty or not passed from calling repository"
              exit 1
            fi

            if [[ -z "${{ secrets.CONFLUENCE_USER }}" ]]; then
              echo "‚ùå CONFLUENCE_USER secret is required for live publishing"
              echo "üêõ Debug: CONFLUENCE_USER is empty or not passed from calling repository"
              exit 1
            fi

            if [[ -z "${{ secrets.CONFLUENCE_API_TOKEN }}" ]]; then
              echo "‚ùå CONFLUENCE_API_TOKEN secret is required for live publishing"
              echo "üêõ Debug: CONFLUENCE_API_TOKEN is empty or not passed from calling repository"
              exit 1
            fi

            echo "‚úÖ All required secrets are configured for live publishing"
          fi

          # Store the effective dry-run status for later steps
          echo "EFFECTIVE_DRY_RUN=$effective_dry_run" >> $GITHUB_ENV

      - name: üîß Prepare Publishing Environment
        run: |
          echo "üîß Preparing publishing environment..."
          echo "üêõ Debug: Environment variable preparation..."

          # Create output directory
          mkdir -p output/confluence

          # Debug: Check what secrets are available
          echo "üêõ Secret availability check:"
          echo "  - CONFLUENCE_URL from secrets: ${{ secrets.CONFLUENCE_URL != '' && 'AVAILABLE' || 'MISSING' }}"
          echo "  - CONFLUENCE_USER from secrets: ${{ secrets.CONFLUENCE_USER != '' && 'AVAILABLE' || 'MISSING' }}"
          echo "  - CONFLUENCE_API_TOKEN from secrets: ${{ secrets.CONFLUENCE_API_TOKEN != '' && 'AVAILABLE' || 'MISSING' }}"

          # Set environment variables (with defaults for missing secrets)
          CONFLUENCE_URL_VALUE="${{ secrets.CONFLUENCE_URL || 'https://your-confluence.atlassian.net' }}"
          CONFLUENCE_USER_VALUE="${{ secrets.CONFLUENCE_USER || 'not-configured' }}"
          CONFLUENCE_TOKEN_VALUE="${{ secrets.CONFLUENCE_API_TOKEN || 'not-configured' }}"

          echo "CONFLUENCE_URL=$CONFLUENCE_URL_VALUE" >> $GITHUB_ENV
          echo "CONFLUENCE_USER=$CONFLUENCE_USER_VALUE" >> $GITHUB_ENV
          echo "CONFLUENCE_TOKEN=$CONFLUENCE_TOKEN_VALUE" >> $GITHUB_ENV
          echo "DRY_RUN=${{ inputs.dry_run }}" >> $GITHUB_ENV
          echo "TARGET_ENV=${{ inputs.target_environment }}" >> $GITHUB_ENV

          echo "üêõ Environment variables set:"
          echo "  - CONFLUENCE_URL: ${CONFLUENCE_URL_VALUE:0:30}..."
          echo "  - CONFLUENCE_USER: ${CONFLUENCE_USER_VALUE:0:10}..."
          echo "  - CONFLUENCE_TOKEN: ${CONFLUENCE_TOKEN_VALUE:0:10}..."

          echo "‚úÖ Environment prepared for ${{ inputs.dry_run == true && 'DRY RUN' || 'LIVE' }} mode"

      - name: üìù Process AAP Documentation
        env:
          CONFLUENCE_URL: ${{ secrets.CONFLUENCE_URL }}
          CONFLUENCE_USER: ${{ secrets.CONFLUENCE_USER }}
          CONFLUENCE_API_TOKEN: ${{ secrets.CONFLUENCE_API_TOKEN }}
        run: |
          echo "üìù Processing AAP documentation templates..."

          # Ensure docs directory exists in calling repo
          mkdir -p calling-repo/docs

          # Copy macros from scripts repository to content repository (only if different repos)
          if [[ "${{ steps.context.outputs.same-repo }}" != "true" ]]; then
            if [ -d "redesigned-guacamole/docs/macros" ]; then
              echo "üìã Copying macros from redesigned-guacamole to calling repository..."
              cp -r redesigned-guacamole/docs/macros calling-repo/docs/
            fi

            # Also copy the main macros.j2 file if it exists
            if [ -f "redesigned-guacamole/docs/macros.j2" ]; then
              echo "üìã Copying macros.j2 from redesigned-guacamole to calling repository..."
              cp redesigned-guacamole/docs/macros.j2 calling-repo/docs/
            fi
          else
            echo "üè† Same repository - macros already available, no copying needed"
          fi

          # Debug: Check if confluence publisher script exists
          echo "üêõ Checking confluence publisher script..."
          echo "  - Current directory: $(pwd)"
          echo "  - Script path: redesigned-guacamole/scripts/confluence_publisher.py"
          echo "  - Script exists: $([ -f "redesigned-guacamole/scripts/confluence_publisher.py" ] && echo 'YES' || echo 'NO')"
          if [ -f "redesigned-guacamole/scripts/confluence_publisher.py" ]; then
            echo "  - Script permissions: $(ls -la redesigned-guacamole/scripts/confluence_publisher.py)"
          fi

          # Verify the script exists (no chmod needed for python3 execution)
          if [ ! -f "redesigned-guacamole/scripts/confluence_publisher.py" ]; then
            echo "‚ùå ERROR: confluence_publisher.py not found at expected path"
            echo "üêõ Available files in scripts directory:"
            if [ -d "redesigned-guacamole/scripts" ]; then
              ls -la redesigned-guacamole/scripts/
            else
              echo "üìÅ Scripts directory not found!"
            fi
            exit 1
          else
            echo "‚úÖ Confluence publisher script found and ready to use"
          fi

          # Run the Python-based Confluence publisher
          if [[ "$EFFECTIVE_DRY_RUN" == "true" ]]; then
            echo "üß™ Running in DRY RUN mode (effective: secrets missing or explicit)"
            python3 redesigned-guacamole/scripts/confluence_publisher.py \
              --dry-run \
              --docs-dir calling-repo/docs \
              --vars-file calling-repo/docs/vars.yaml
          else
            echo "üöÄ Running in LIVE mode"
            python3 redesigned-guacamole/scripts/confluence_publisher.py \
              --docs-dir calling-repo/docs \
              --vars-file calling-repo/docs/vars.yaml \
              --confluence-url "${CONFLUENCE_URL:-https://your-confluence.atlassian.net}" \
              --confluence-user "${CONFLUENCE_USER:-not-configured}" \
              --confluence-token "${CONFLUENCE_TOKEN:-not-configured}"
          fi

      - name: üìä Generate Publishing Report
        if: always()
        run: |
          echo "üìä Generating publishing report..."

          # Create a summary report
          cat > output/confluence/publishing_report.md << EOF
          # üìö Confluence Publishing Report

          ## üìã Publishing Details
          - **Target Environment**: ${{ inputs.target_environment }}
          - **Dry Run**: ${{ inputs.dry_run }}
          - **Timestamp**: $(date -u +"%Y-%m-%d %H:%M:%S UTC")
          - **Git Reference**: ${{ github.ref_name }}
          - **Commit SHA**: ${{ github.sha }}
          - **Workflow Run**: ${{ github.run_number }}
          - **Repository Context**: ${{ steps.context.outputs.same-repo == 'true' && 'Same Repository' || 'Remote Call' }}

          ## üîó Links
          - **Repository**: ${{ github.server_url }}/${{ github.repository }}
          - **Workflow Run**: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}

          ## üìà Status
          ${{ inputs.dry_run == true && 'üß™ **DRY RUN** - No actual changes were made' || '‚úÖ **LIVE RUN** - Documentation was published to Confluence' }}
          EOF

          echo "‚úÖ Publishing report generated"
          cat output/confluence/publishing_report.md

      - name: üì§ Upload Publishing Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: confluence-publishing-report-${{ github.run_id }}
          path: |
            output/confluence/
          retention-days: 30
          if-no-files-found: warn

      - name: üîî Publishing Summary
        if: always()
        run: |
          echo "## üìö Confluence Publishing Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üìã Details" >> $GITHUB_STEP_SUMMARY
          echo "- **Target Environment**: ${{ inputs.target_environment }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Dry Run**: ${{ inputs.dry_run }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Git Reference**: ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Commit**: \`${{ github.sha }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Repository Context**: ${{ steps.context.outputs.same-repo == 'true' && 'Same Repository' || 'Remote Call' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [[ "$EFFECTIVE_DRY_RUN" == "true" ]]; then
            echo "### üß™ Dry Run Results" >> $GITHUB_STEP_SUMMARY
            echo "- No actual changes were made to Confluence" >> $GITHUB_STEP_SUMMARY
            echo "- Documentation processing completed successfully" >> $GITHUB_STEP_SUMMARY
            if [[ "${{ inputs.dry_run }}" != "true" ]]; then
              echo "- **Auto-enabled due to missing Confluence secrets**" >> $GITHUB_STEP_SUMMARY
              echo "- Add CONFLUENCE_URL, CONFLUENCE_USER, and CONFLUENCE_API_TOKEN secrets for live publishing" >> $GITHUB_STEP_SUMMARY
            fi
            echo "- Ready for live publishing when secrets are configured" >> $GITHUB_STEP_SUMMARY
          else
            echo "### ‚úÖ Publishing Results" >> $GITHUB_STEP_SUMMARY
            echo "- Documentation has been published to Confluence" >> $GITHUB_STEP_SUMMARY
            echo "- Check Confluence space for updated content" >> $GITHUB_STEP_SUMMARY
          fi

  # Comprehensive report - always runs to provide summary
  comprehensive-report:
    name: üìä Generate Comprehensive Report
    # This job runs after all checks are complete, including publish which now runs in parallel
    needs: [super-linter, security, detect-changes, publish]
    if: always()  # Always run to report on success or failure
    runs-on: ubuntu-latest
    steps:
      - name: ÔøΩ Checkout Code (for scripts)
        uses: actions/checkout@v4
        with:
          ref: ${{ inputs.branch_name || github.ref }}

      - name: ÔøΩüì• Download all artifacts
        uses: actions/download-artifact@v4
        with:
          # Download all artifacts into a 'reports' directory
          path: ./reports
        continue-on-error: true

      - name: üêç Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'
          # Don't use cache to avoid dependency on calling repo's requirements

      - name: üì¶ Install Dependencies
        run: |
          python -m pip install --upgrade pip
          # Only install dependencies if they exist in the calling repository
          if [ -f requirements.txt ]; then
            echo "Found requirements.txt in calling repository, installing dependencies..."
            pip install -r requirements.txt
          elif [ -f pyproject.toml ]; then
            echo "Found pyproject.toml in calling repository, installing dependencies..."
            pip install -e .
          else
            echo "No requirements.txt or pyproject.toml found in calling repository"
            echo "Installing minimal dependencies for workflow functionality..."
          fi

          # Always install tools needed for auto-fixing and analysis
          echo "Installing workflow tools..."
          pip install black isort PyYAML requests

      - name: üìù Generate SARIF Security Reports Summary
        if: always()
        run: |
          python3 << 'EOF'
          import json
          import os
          import glob
          import sys

          def parse_sarif(file_path):
              if not os.path.exists(file_path):
                  return '| N/A | No report found | This scan may have failed to produce a report. | N/A |\n', 0

              report_md = ''
              results = {}
              total_issues = 0

              try:
                  with open(file_path, 'r', encoding='utf-8') as f:
                      sarif_data = json.load(f)
                      for run in sarif_data.get('runs', []):
                          for result in run.get('results', []):
                              total_issues += 1
                              rule_id = result.get('ruleId', 'unknown')
                              message = result.get('message', {}).get('text', 'No description available')
                              level = result.get('level', 'warning').upper()
                              key = (level, rule_id, message)
                              results[key] = results.get(key, 0) + 1
              except (json.JSONDecodeError, KeyError, TypeError) as e:
                  return f'| ERROR | PARSE_ERROR | Failed to parse SARIF file: {str(e)} | N/A |\n', 0

              if not results:
                  report_md += '| ‚úÖ | None | No issues found! | 0 |\n'
              else:
                  for (level, rule, msg), count in sorted(results.items()):
                      msg_escaped = msg.replace('|', '\\|').replace('\n', ' ').replace('\r', '').replace('`', '\\`')
                      report_md += f'| {level} | \\`{rule}\\` | {msg_escaped} | {count} |\n'

              return report_md, total_issues

          # Main execution
          summary_file = os.getenv('GITHUB_STEP_SUMMARY')
          if not summary_file:
              print('ERROR: GITHUB_STEP_SUMMARY environment variable not set', file=sys.stderr)
              sys.exit(1)

          reports_dir = './reports'
          overall_total = 0

          # Dynamically discover all SARIF reports
          all_reports = {}
          for p in glob.glob(os.path.join(reports_dir, '**', '*.sarif'), recursive=True):
              name = os.path.basename(os.path.dirname(p)).replace('-', ' ').replace('_', ' ').title()
              if name:
                  all_reports[name] = p

          print(f'Processing {len(all_reports)} report types...')
          print(f'Reports directory: {reports_dir}')
          print(f'Summary will be written to: {summary_file}')

          # Check if reports directory exists
          if not os.path.exists(reports_dir):
              print(f'WARNING: Reports directory {reports_dir} does not exist')
              print('This is normal if no artifacts were generated or downloaded.')
              with open(summary_file, 'a') as f:
                  f.write('## üìä Security & Quality Reports Summary\n\n')
                  f.write('### ‚ö†Ô∏è No Reports Found\n\n')
                  f.write('The reports directory was not found. This may indicate:\n')
                  f.write('- No artifacts were generated by the scanning jobs\n')
                  f.write('- All scans passed without issues (no SARIF files created)\n')
                  f.write('- Artifact download failed or was skipped\n\n')
                  f.write('**This is often normal** - it means no security or linting issues were detected!\n\n')
              print('‚úÖ Summary generated with no reports found message')
              sys.exit(0)

          # Check if directory is empty
          all_files = glob.glob(os.path.join(reports_dir, '**', '*'), recursive=True)
          report_files = [f for f in all_files if f.endswith('.sarif')]

          if not report_files:
              print(f'INFO: Reports directory exists but contains no SARIF files')
              print(f'Found {len(all_files)} total files in reports directory')
              with open(summary_file, 'a') as f:
                  f.write('## üìä Security & Quality Reports Summary\n\n')
                  f.write('### ‚úÖ No Issues Found\n\n')
                  f.write('Reports directory exists but no SARIF reports were found.\n')
                  f.write('This typically means all security and linting scans passed successfully!\n\n')
                  if len(all_files) > 0:
                      f.write(f'üìÅ **Files found in reports directory**: {len(all_files)}\n')
                      f.write('(These may be log files or other artifacts)\n\n')
              print('‚úÖ Summary generated with no issues found message')
              sys.exit(0)

          # Process each report
          with open(summary_file, 'a') as f:
              f.write('## üìä Security & Quality Reports Summary\n\n')

          for tool_name, report_path in all_reports.items():
              print(f'Processing {tool_name} report: {report_path}')

              with open(summary_file, 'a') as f:
                  f.write(f'### {tool_name} Report\n')
                  f.write('| Severity | Rule ID | Description | Total |\n')
                  f.write('|----------|---------|-------------|-------|\n')

              report_content, issue_count = parse_sarif(report_path)
              overall_total += issue_count

              with open(summary_file, 'a') as f:
                  f.write(report_content)
                  f.write('\n')

              print(f'  - Found {issue_count} issues in {tool_name}')

          # Final summary
          with open(summary_file, 'a') as f:
              f.write('\n---\n\n')
              if overall_total == 0:
                  f.write('### üéâ Excellent! No Issues Found\n\n')
                  f.write('All security and quality scans completed successfully with no issues detected.\n')
              else:
                  f.write(f'### üìä Total Issues Found: {overall_total}\n\n')
                  f.write('Please review the individual reports above for detailed information.\n')

          print(f'‚úÖ Summary generation complete. Total issues across all tools: {overall_total}')
          EOF

      - name: üìä Execution Summary
        if: always()
        run: |
          {
            echo "## üöÄ Workflow Execution Summary"
            echo ""
            echo "### üîç Change Detection Results"
            echo "- **Repository**: ${{ github.repository }}"
            echo "- **External call**: ${{ needs.detect-changes.outputs.external-call }}"
            echo "- **Documentation changed**: ${{ needs.detect-changes.outputs.docs-changed }}"
            echo "- **Ansible files changed**: ${{ needs.detect-changes.outputs.ansible-changed }}"
            echo "- **Python files changed**: ${{ needs.detect-changes.outputs.python-changed }}"
            echo "- **Workflow files changed**: ${{ needs.detect-changes.outputs.workflows-changed }}"
            echo "- **Any code changed**: ${{ needs.detect-changes.outputs.any-code-changed }}"
            echo ""
            echo "### üìä Job Execution Status"
            echo "- **Super Linter**: ${{ needs.super-linter.result }}"
            echo "- **Security Scan**: ${{ needs.security.result }}"
            echo "- **Publishing**: ${{ needs.publish.result || 'skipped (no docs changes or not main branch)' }}"
          } >> "$GITHUB_STEP_SUMMARY"
