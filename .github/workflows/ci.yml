---
name: üöÄ CI/CD Pipeline

on:
  workflow_call:
    inputs:
      full_scan:
        description: 'Run full codebase scan (not just changed files)'
        required: false
        type: boolean
        default: true
      branch_name:
        description: 'Branch name to checkout'
        required: false
        type: string
        default: ''
    secrets:
      CONFLUENCE_URL:
        required: false
      CONFLUENCE_USER:
        required: false
      CONFLUENCE_API_TOKEN:
        required: false

jobs:
  super-linter:
    name: üîç Super Linter
    runs-on: ubuntu-latest

    permissions:
      contents: read
      packages: read
      statuses: write
      security-events: write

    steps:
      - name: üì• Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: üîß Configure Environment
        id: config
        run: |
          # Dynamic branch detection
          if [ "${{ github.event.repository.default_branch }}" != "" ]; then
            echo "default_branch=${{ github.event.repository.default_branch }}" >> $GITHUB_OUTPUT
          else
            echo "default_branch=main" >> $GITHUB_OUTPUT
          fi

      - name: üîç Verify Linter Configuration Files
        run: |
          echo "üîç Checking for linter configuration files..."

          # Check for .ansible-lint
          if [ -f ".ansible-lint" ]; then
            echo "‚úÖ .ansible-lint found ($(wc -l < .ansible-lint) lines)"
            echo "üìÑ .ansible-lint preview:"
            head -5 .ansible-lint
          else
            echo "‚ùå .ansible-lint not found"
          fi

          # Check for .yamllint
          if [ -f ".yamllint" ]; then
            echo "‚úÖ .yamllint found ($(wc -l < .yamllint) lines)"
          else
            echo "‚ùå .yamllint not found"
          fi

          # Check for markdownlint config (prefer .json)
          if [ -f ".markdownlint.json" ]; then
            echo "‚úÖ .markdownlint.json found ($(wc -l < .markdownlint.json) lines)"
          elif [ -f ".markdownlint.yml" ]; then
            echo "‚úÖ .markdownlint.yml found ($(wc -l < .markdownlint.yml) lines)"
          else
            echo "‚ùå No markdownlint config found"
          fi

          # Check for any conflicting config files
          if [ -f ".markdownlint.json" ] && [ -f ".markdownlint.yml" ]; then
            echo "‚ö†Ô∏è  Both .markdownlint.json and .markdownlint.yml found - Super Linter will prefer .json"
          fi

          echo "üéØ Super Linter will auto-detect all configuration files at repo root"

      - name: üîç Run Super Linter
        id: super-linter
        uses: super-linter/super-linter@v5
        env:
          # Use dynamically generated configuration
          ENV_FILE: .github/super-linter-dynamic.env
          
          # GitHub token (required for API access)
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: üìä Analyze Auto-fixes Applied
        if: always()
        id: autofix-analysis
        run: |
          echo "üîç Analyzing auto-fixes applied by Super Linter..."

          # Initialize counters
          total_fixes=0
          yaml_fixes=0
          ansible_fixes=0
          python_fixes=0
          shell_fixes=0
          markdown_fixes=0
          json_fixes=0

          # Function to count fixes in git diff
          count_file_changes() {
            local file_pattern="$1"
            local category="$2"

            if git diff --name-only | grep -E "$file_pattern" > /dev/null 2>&1; then
              local changed_files=$(git diff --name-only | grep -E "$file_pattern" | wc -l)
              local total_changes=$(git diff --numstat | grep -E "$file_pattern" | awk '{sum += $1 + $2} END {print sum+0}')

              echo "üìù $category fixes: $changed_files files, $total_changes changes"
              return $total_changes
            else
              echo "üìù $category fixes: 0 files, 0 changes"
              return 0
            fi
          }

          # Check if there are any changes from auto-fix
          if git diff --quiet; then
            echo "‚úÖ No auto-fixes were needed - code already compliant!"
            echo "autofix_needed=false" >> $GITHUB_OUTPUT
            echo "total_fixes=0" >> $GITHUB_OUTPUT
          else
            echo "üîß Auto-fixes were applied!"
            echo "autofix_needed=true" >> $GITHUB_OUTPUT

            # Count fixes by category
            count_file_changes "\\.ya?ml$" "YAML"; yaml_fixes=$?
            count_file_changes "\\.py$" "Python"; python_fixes=$?
            count_file_changes "\\.sh$" "Shell/Bash"; shell_fixes=$?
            count_file_changes "\\.md$" "Markdown"; markdown_fixes=$?
            count_file_changes "\\.json$" "JSON"; json_fixes=$?

            # Special handling for Ansible (subset of YAML)
            if git diff --name-only | grep -E "(playbook|tasks|handlers|vars).*\\.ya?ml$|site\\.ya?ml$" > /dev/null 2>&1; then
              ansible_fixes=$(git diff --numstat | grep -E "(playbook|tasks|handlers|vars).*\\.ya?ml$|site\\.ya?ml$" | awk '{sum += $1 + $2} END {print sum+0}')
              echo "üìù Ansible fixes: $ansible_fixes changes"
            fi

            total_fixes=$((yaml_fixes + python_fixes + shell_fixes + markdown_fixes + json_fixes))

            # Output for next steps
            echo "total_fixes=$total_fixes" >> $GITHUB_OUTPUT
            echo "yaml_fixes=$yaml_fixes" >> $GITHUB_OUTPUT
            echo "ansible_fixes=$ansible_fixes" >> $GITHUB_OUTPUT
            echo "python_fixes=$python_fixes" >> $GITHUB_OUTPUT
            echo "shell_fixes=$shell_fixes" >> $GITHUB_OUTPUT
            echo "markdown_fixes=$markdown_fixes" >> $GITHUB_OUTPUT
            echo "json_fixes=$json_fixes" >> $GITHUB_OUTPUT

            # Show detailed diff summary
            echo ""
            echo "üìã Detailed changes by file:"
            git diff --name-status | while read status file; do
              if [ "$status" = "M" ]; then
                changes=$(git diff --numstat "$file" | awk '{print $1 + $2}')
                echo "  üìÑ $file: $changes changes"
              fi
            done
          fi

      - name: üíæ Commit Auto-fixes
        if: steps.autofix-analysis.outputs.autofix_needed == 'true'
        run: |
          echo "üíæ Committing auto-fixes..."

          # Configure git (use GitHub Actions bot)
          git config --local user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"

          # Add all changes
          git add .

          # Create detailed commit message
          commit_msg="ü§ñ Auto-fix: Applied ${{ steps.autofix-analysis.outputs.total_fixes }} linting fixes

          Auto-fixes applied by Super Linter:
          - YAML fixes: ${{ steps.autofix-analysis.outputs.yaml_fixes }}
          - Ansible fixes: ${{ steps.autofix-analysis.outputs.ansible_fixes }}
          - Python fixes: ${{ steps.autofix-analysis.outputs.python_fixes }}
          - Shell fixes: ${{ steps.autofix-analysis.outputs.shell_fixes }}
          - Markdown fixes: ${{ steps.autofix-analysis.outputs.markdown_fixes }}
          - JSON fixes: ${{ steps.autofix-analysis.outputs.json_fixes }}

          Automated by: ${{ github.workflow }} #${{ github.run_number }}
          Triggered by: ${{ github.event_name }} on ${{ github.ref_name }}"

          # Commit changes
          git commit -m "$commit_msg"

          # Push changes back to the branch
          git push origin ${{ github.ref_name }}

          echo "‚úÖ Auto-fixes committed and pushed!"

      - name: üßπ Sanitize Super Linter Logs
        if: always()
        run: |
          echo "üßπ Starting log sanitization process..."
          
          # Check if log file exists and sanitize it
          if [ -f "super-linter.log" ]; then
            echo "‚úÖ Super Linter log file found: $(ls -la super-linter.log)"
            echo "üìä Original log file size: $(wc -l < super-linter.log) lines, $(du -h super-linter.log | cut -f1) size"

            # Create a backup
            cp super-linter.log super-linter.log.original
            echo "‚úÖ Backup created: $(ls -la super-linter.log.original)"

            # Enhanced masking - Remove or mask potential sensitive patterns
            echo "üé≠ Applying comprehensive masking rules..."
            
            # GitHub token patterns (various formats)
            sed -i 's/ghp_[a-zA-Z0-9]\{36\}/***GITHUB_TOKEN***/g' super-linter.log
            sed -i 's/gho_[a-zA-Z0-9]\{36\}/***GITHUB_TOKEN***/g' super-linter.log
            sed -i 's/ghu_[a-zA-Z0-9]\{36\}/***GITHUB_TOKEN***/g' super-linter.log
            sed -i 's/ghs_[a-zA-Z0-9]\{36\}/***GITHUB_TOKEN***/g' super-linter.log
            sed -i 's/github_pat_[a-zA-Z0-9_]\{82\}/***GITHUB_PAT***/g' super-linter.log
            
            # Atlassian/Confluence tokens
            sed -i 's/ATATT[a-zA-Z0-9+/=]\{40,\}/***ATLASSIAN_TOKEN***/g' super-linter.log
            sed -i 's/ATCTT[a-zA-Z0-9+/=]\{40,\}/***ATLASSIAN_TOKEN***/g' super-linter.log
            sed -i 's/ATBT[a-zA-Z0-9+/=]\{40,\}/***ATLASSIAN_TOKEN***/g' super-linter.log
            
            # Authentication headers and tokens
            sed -i 's/Basic [a-zA-Z0-9+/=]\{20,\}/Basic ***ENCODED_CREDENTIALS***/g' super-linter.log
            sed -i 's/Bearer [a-zA-Z0-9+/=]\{20,\}/Bearer ***TOKEN***/g' super-linter.log
            sed -i 's/Authorization: [^[:space:]]\+/Authorization: ***MASKED***/g' super-linter.log
            
            # API keys and secrets (various patterns)
            sed -i 's/[aA][pP][iI][_-][kK][eE][yY][_-]*[:=][[:space:]]*['\''"][a-zA-Z0-9+/=]\{20,\}['\''"]*/API_KEY: ***MASKED***/g' super-linter.log
            sed -i 's/[sS][eE][cC][rR][eE][tT][_-]*[:=][[:space:]]*['\''"][a-zA-Z0-9+/=]\{20,\}['\''"]*/SECRET: ***MASKED***/g' super-linter.log
            sed -i 's/[pP][aA][sS][sS][wW][oO][rR][dD][_-]*[:=][[:space:]]*['\''"][a-zA-Z0-9+/=]\{8,\}['\''"]*/PASSWORD: ***MASKED***/g' super-linter.log
            
            # Generic long base64 patterns that could be credentials (but be careful not to mask normal data)
            sed -i 's/[a-zA-Z0-9+/=]\{60,\}/***POTENTIAL_ENCODED_DATA***/g' super-linter.log
            
            # URLs with embedded credentials
            sed -i 's|https://[^:]*:[^@]*@|https://***USER***:***PASS***@|g' super-linter.log
            sed -i 's|http://[^:]*:[^@]*@|http://***USER***:***PASS***@|g' super-linter.log
            
            # Common environment variable patterns that might contain secrets
            sed -i 's/CONFLUENCE_AUTH=[^[:space:]]*/CONFLUENCE_AUTH=***MASKED***/g' super-linter.log
            sed -i 's/CONFLUENCE_URL=[^[:space:]]*/CONFLUENCE_URL=***MASKED***/g' super-linter.log
            sed -i 's/GITHUB_TOKEN=[^[:space:]]*/GITHUB_TOKEN=***MASKED***/g' super-linter.log
            
            # AWS and other cloud provider keys
            sed -i 's/AKIA[0-9A-Z]\{16\}/***AWS_ACCESS_KEY***/g' super-linter.log
            sed -i 's/[0-9a-zA-Z/+]\{40\}/***AWS_SECRET_KEY***/g' super-linter.log
            
            # SSH keys
            sed -i 's/-----BEGIN [A-Z ]*PRIVATE KEY-----.*-----END [A-Z ]*PRIVATE KEY-----/***SSH_PRIVATE_KEY***/g' super-linter.log
            
            echo "‚úÖ Masking rules applied successfully"
            echo "üìä Sanitized log file size: $(wc -l < super-linter.log) lines, $(du -h super-linter.log | cut -f1) size"

            # Verify sanitization by checking for common patterns
            echo "üîç Verifying sanitization effectiveness..."
            potential_leaks=0
            
            # Check for remaining sensitive patterns
            if grep -q "ghp_\|gho_\|ghu_\|ghs_\|github_pat_" super-linter.log; then
              echo "‚ö†Ô∏è Warning: Potential GitHub tokens still found"
              potential_leaks=$((potential_leaks + 1))
            fi
            
            if grep -q "ATATT\|ATCTT\|ATBT" super-linter.log; then
              echo "‚ö†Ô∏è Warning: Potential Atlassian tokens still found"
              potential_leaks=$((potential_leaks + 1))
            fi
            
            if grep -qE "Bearer [a-zA-Z0-9+/=]{20,}|Basic [a-zA-Z0-9+/=]{20,}" super-linter.log; then
              echo "‚ö†Ô∏è Warning: Potential authentication headers still found"
              potential_leaks=$((potential_leaks + 1))
            fi
            
            if [ $potential_leaks -eq 0 ]; then
              echo "‚úÖ Sanitization verification passed - no obvious sensitive patterns detected"
            else
              echo "‚ö†Ô∏è Sanitization verification found $potential_leaks potential issues - please review"
            fi

            # Show a safe sample of sanitized content for debugging
            echo "üìã Sanitized log sample (first 20 lines, excluding any masked content):"
            head -20 super-linter.log | grep -v "\*\*\*" | head -10 || echo "All sample lines contain masked content"
            
            echo "üìã Sanitized log sample (last 10 lines, excluding any masked content):"
            tail -20 super-linter.log | grep -v "\*\*\*" | head -10 || echo "All sample lines contain masked content"
            
          else
            echo "‚ùå Super Linter log file not found"
            echo "üìÇ Current directory contents:"
            ls -la
            echo "üìÇ Looking for any log files:"
            find . -name "*.log" -type f 2>/dev/null || echo "No .log files found"
            
            # Create an empty log file so upload doesn't fail
            echo "üîß Creating placeholder log file for upload"
            echo "Super Linter log file was not generated or not found." > super-linter.log
            echo "This may indicate that Super Linter failed early or encountered an error." >> super-linter.log
            echo "Check the Super Linter step output for more details." >> super-linter.log
            echo "Generated at: $(date)" >> super-linter.log
          fi

      - name: ÔøΩ Analyze Linting Rule Violations
        if: always()
        id: rule-analysis
        run: |
          echo "üîç Analyzing linting rule violations..."

          if [ -f "super-linter.log" ]; then
            echo "‚úÖ Super Linter log file found"
            echo "üìä Log file size: $(wc -l < super-linter.log) lines"
            
            # Show sample of log to debug format
            echo "üîç First 10 lines of super-linter.log:"
            head -10 super-linter.log
            echo ""
            echo "üîç Last 10 lines of super-linter.log:"
            tail -10 super-linter.log
            echo ""
            
            # Extract rule violations and count them
            echo "Extracting rule violations from logs..."

            # Create temporary files for processing
            temp_violations="/tmp/violations.txt"
            temp_descriptions="/tmp/descriptions.txt"
            temp_counts="/tmp/rule_counts.txt"

            # Extract lines with rule violations (those ending with [rulename])
            echo "üîç Looking for lines ending with [rulename]..."
            grep -E '\[[a-zA-Z0-9_-]+\]$' super-linter.log > "$temp_violations" || echo "No rule violations found in specific format"
            
            if [ -s "$temp_violations" ]; then
              echo "‚úÖ Found $(wc -l < "$temp_violations") violation lines"
              echo "üîç Sample violations found:"
              head -3 "$temp_violations"
              echo ""
            else
              echo "‚ùå No violations found in expected format"
              echo "üîç Searching for alternative patterns..."
              echo "Lines containing 'shellcheck':"
              grep -i "shellcheck" super-linter.log | head -3 || echo "None found"
              echo "Lines containing 'actionlint':"
              grep -i "actionlint" super-linter.log | head -3 || echo "None found"
              echo "Lines containing any brackets:"
              grep '\[.*\]' super-linter.log | head -3 || echo "None found"
              echo ""
            fi

            if [ -s "$temp_violations" ]; then
              echo "Processing violation details..."

              # Process each violation to extract linter, description, and rule
              while IFS= read -r line; do
                # Extract the rule/linter name (last item in brackets)
                rule=$(echo "$line" | sed -E 's/.*\[([a-zA-Z0-9_-]+)\]$/\1/')
                
                # Extract the description - different patterns for different linters
                if [[ "$line" == *"shellcheck reported issue"* ]]; then
                  # For shellcheck: extract description after the rule code
                  description=$(echo "$line" | sed -E 's/.*SC[0-9]+:[^:]+:[^:]+:[^:]+: ([^[]+) \[shellcheck\]$/\1/' | sed 's/^ *//' | sed 's/ *$//')
                  linter="shellcheck"
                elif [[ "$line" == *"expression"* ]]; then
                  # For actionlint expression warnings
                  description=$(echo "$line" | sed -E 's/^[^:]*:[^:]*:[^:]*: ([^[]+) \[expression\]$/\1/' | sed 's/^ *//' | sed 's/ *$//')
                  linter="actionlint"
                else
                  # For other linters, try to extract description before the rule
                  description=$(echo "$line" | sed -E 's/^[^:]*:[^:]*:[^:]*: ([^[]+) \[[^]]+\]$/\1/' | sed 's/^ *//' | sed 's/ *$//')
                  linter="$rule"
                fi

                # Clean up description - remove extra whitespace and truncate if too long
                description=$(echo "$description" | sed 's/  */ /g' | cut -c1-80)
                
                # If description extraction failed, use a fallback
                if [[ -z "$description" || "$description" == "$line" ]]; then
                  description="Rule violation detected"
                fi

                # Create a combined key for grouping: linter|description
                echo "${linter}|${description}" >> "$temp_descriptions"
              done < "$temp_violations"

              # Count occurrences of each linter|description combination
              sort "$temp_descriptions" | uniq -c | sort -nr > "$temp_counts"

              # Count total violations
              total_violations=$(wc -l < "$temp_violations")
              unique_rules=$(wc -l < "$temp_counts")

              echo "Found $total_violations total violations across $unique_rules unique rule types"

              # Generate summary for outputs
              echo "total_violations=$total_violations" >> $GITHUB_OUTPUT
              echo "unique_rules=$unique_rules" >> $GITHUB_OUTPUT

              # Create detailed rule breakdown
              echo "violations_found=true" >> $GITHUB_OUTPUT

              # Generate markdown table data
              rules_table=""
              total_count=0
              
              while IFS= read -r line; do
                # Extract count (first field)
                count=$(echo "$line" | awk '{print $1}')
                # Extract everything after the count
                rest=$(echo "$line" | awk '{$1=""; print $0}' | sed 's/^ *//')
                # Split on pipe character
                linter=$(echo "$rest" | cut -d'|' -f1)
                description=$(echo "$rest" | cut -d'|' -f2-)
                
                # Determine severity icon based on linter and patterns
                severity_icon="‚ö†Ô∏è"
                if [[ "$linter" == "shellcheck" ]]; then
                  if [[ "$description" == *"warning"* ]]; then
                    severity_icon="üü°"
                  elif [[ "$description" == *"error"* ]]; then
                    severity_icon="üî¥"
                  else
                    severity_icon="üîµ"
                  fi
                elif [[ "$linter" == "actionlint" ]]; then
                  severity_icon="üü°"
                elif [[ "$description" == *"error"* ]] || [[ "$description" == *"security"* ]]; then
                  severity_icon="üî¥"
                elif [[ "$description" == *"warning"* ]] || [[ "$description" == *"style"* ]]; then
                  severity_icon="üü°"
                elif [[ "$description" == *"info"* ]]; then
                  severity_icon="üîµ"
                fi

                rules_table="$rules_table| $severity_icon **$linter** | $description | $count |\n"
                total_count=$((total_count + count))
              done < "$temp_counts"

              # Add total row
              rules_table="$rules_table| üìä **TOTAL** | **All violations** | **$total_count** |\n"

              # Save for use in summary step
              echo -e "$rules_table" > /tmp/rules_table.md

              echo "‚úÖ Rule analysis completed with detailed descriptions"
            else
              echo "No rule violations found or different log format"
              echo "violations_found=false" >> $GITHUB_OUTPUT
              echo "total_violations=0" >> $GITHUB_OUTPUT
              echo "unique_rules=0" >> $GITHUB_OUTPUT
            fi

            # Clean up temp files
            rm -f "$temp_violations" "$temp_descriptions" "$temp_counts"
          else
            echo "‚ö†Ô∏è Super Linter log file not found for analysis"
            echo "violations_found=false" >> $GITHUB_OUTPUT
            echo "total_violations=0" >> $GITHUB_OUTPUT
            echo "unique_rules=0" >> $GITHUB_OUTPUT
          fi

      - name: üì§ Upload Sanitized Super Linter Logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: super-linter-logs-sanitized-${{ github.run_id }}
          path: super-linter.log
          retention-days: 30
          if-no-files-found: warn

      - name: üì§ Upload Original Super Linter Logs (Debug Only)
        if: always() && github.event.inputs.full_scan == 'true'  # Only upload original logs for full scans
        uses: actions/upload-artifact@v4
        with:
          name: super-linter-logs-original-${{ github.run_id }}
          path: super-linter.log.original
          retention-days: 7  # Shorter retention for potentially sensitive logs
          if-no-files-found: warn

      - name: üì§ Upload Dynamic Configuration (Debug)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: super-linter-config-${{ github.run_id }}
          path: .github/super-linter-dynamic.env
          retention-days: 7
          if-no-files-found: warn

  # Enhanced security scanning
  security:
    name: üõ°Ô∏è Security Scan
    runs-on: ubuntu-latest
    permissions:
      actions: read
      contents: read
      security-events: write
    steps:
      - name: üì• Checkout Code
        uses: actions/checkout@v4

      - name: üîç Run DevSkim Scanner
        uses: microsoft/DevSkim-Action@v1

      - name: üì§ Upload DevSkim SARIF
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: devskim-results.sarif

      - name: ‚ò¢Ô∏è Run Trivy Vulnerability Scanner
        uses: aquasecurity/trivy-action@v0.10.0
        with:
          scan-type: 'fs'
          ignore-unfixed: true
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH'

      - name: üì§ Upload Trivy Scan Results
        uses: actions/upload-artifact@v4
        with:
          name: trivy-results
          path: trivy-results.sarif
      - name: üîê Advanced Secret Detection
        run: |
          echo "üîç Running security validation..."

          # Check for potential secrets (excluding false positives)
          echo "Checking for potential hardcoded secrets..."
          if grep -rE "(password|secret|api_key|auth_token|private_key):\s*['\"]?[A-Za-z0-9+/=]{10,}" . \
             --include="*.yml" --include="*.yaml" --include="*.py" --include="*.sh" \
             --exclude-dir=.git --exclude-dir=.github \
             --exclude="*example*" --exclude="*template*" \
             | grep -v "YOUR_.*_HERE\|test:test\|example\|template\|#.*token\|#.*secret\|README"; then
            echo "‚ö†Ô∏è Potential secrets found - please review"
            exit 1
          else
            echo "‚úÖ No obvious secrets detected"
          fi

      - name: üîí File Permissions Check
        run: |
          echo "üîí Checking file permissions..."

          # Check for world-writable files
          if find . -name "*.yml" -o -name "*.yaml" -o -name "*.py" -o -name "*.sh" | xargs ls -la | grep "^-.......rw"; then
            echo "‚ùå World-writable files found"
            find . -name "*.yml" -o -name "*.yaml" -o -name "*.py" -o -name "*.sh" | xargs ls -la | grep "^-.......rw"
            exit 1
          else
            echo "‚úÖ File permissions look secure"
          fi

      - name: üõ°Ô∏è Security Summary
        if: always()
        run: |
          echo "## üõ°Ô∏è Security Validation Summary" >> $GITHUB_STEP_SUMMARY
          echo "‚úÖ **DevSkim scan completed**" >> $GITHUB_STEP_SUMMARY
          echo "‚úÖ **Secret detection completed**" >> $GITHUB_STEP_SUMMARY
          echo "‚úÖ **File permissions checked**" >> $GITHUB_STEP_SUMMARY

  ansible-syntax-check:
    name:  Ansible Syntax Check
    runs-on: ubuntu-latest
    steps:
      - name: üì• Checkout Code
        uses: actions/checkout@v4

      - name: üîß Set up Python & Install Dependencies
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      - run: |
          python -m pip install --upgrade pip
          pip install ansible-lint # We only need ansible-lint

      - name: üé≠ Run Ansible Lint
        id: lint # Give this step an ID to reference its outcome
        run: |
          # This single command lints all files, respects .ansible-lint config,
          # and creates the SARIF report.
          ansible-lint --sarif-file=ansible-results.sarif
        continue-on-error: true # Allow the workflow to continue to the upload step

      - name: üì§ Upload Ansible-Lint SARIF Report
        if: always() # Always run this step to upload the report
        uses: actions/upload-artifact@v4
        with:
          name: ansible-lint-results
          path: ansible-results.sarif

      - name: üìã Check Linting Results
        if: steps.lint.outcome == 'failure'
        run: |
          echo "‚ùå Ansible-lint found issues."
          exit 1 # Explicitly fail the job if the linting step failed
        continue-on-error: true # Allow the workflow to continue even if linting fails

  publish:
    name: üöÄ Publish to Confluence
    # Your conditions for running the job remain the same
    needs: [super-linter, security, ansible-syntax-check]
    if: >
      github.event_name == 'push' &&
      (github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/heads/release/') || startsWith(github.ref, 'refs/heads/hotfix/')) &&
      needs.ansible-syntax-check.result == 'success'

    # üí° 1. The 'uses' key now calls your reusable workflow.
    # The runs-on and steps are removed.
    # Make sure to use the correct branch name or tag (e.g., @main).
    uses: KhalilGibrotha/bug-free-fiesta/.github/workflows/publish-docs.yml@main

    # üí° 2. Pass the required secrets to the reusable workflow.
    secrets:
      CONFLUENCE_URL: ${{ secrets.CONFLUENCE_URL }}
      CONFLUENCE_USER: ${{ secrets.CONFLUENCE_USER }}
      CONFLUENCE_API_TOKEN: ${{ secrets.CONFLUENCE_API_TOKEN }}

# In ci.yml, this replaces all other summary/report jobs

  comprehensive-report:
    name: üìä Generate Comprehensive Report
    # This job runs after all checks are complete
    needs: [security, ansible-syntax-check]
    if: always() # Always run to report on success or failure
    runs-on: ubuntu-latest
    steps:
      - name: üì• Download all artifacts
        uses: actions/download-artifact@v4
        with:
          # Download all artifacts into a 'reports' directory
          path: ./reports
        continue-on-error: true

      - name: üêç Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: üìù Generate Summary from All Reports
        run: |
          import json
          import os
          import glob

          summary_file = os.getenv('GITHUB_STEP_SUMMARY')
          reports_dir = './reports'
          overall_total = 0

          # Helper function to parse a SARIF file and return a formatted string
          def parse_sarif(file_path):
              if not os.path.exists(file_path):
                  return "| N/A | No report found | This scan may have failed to produce a report. | N/A |\n", 0

              report_md = ""
              results = {}
              total_issues = 0
              with open(file_path) as f:
                  sarif_data = json.load(f)
                  for run in sarif_data.get('runs', []):
                      for result in run.get('results', []):
                          total_issues += 1
                          rule_id = result['ruleId']
                          message = result['message']['text']
                          level = result.get('level', 'warning').upper()
                          key = (level, rule_id, message)
                          results[key] = results.get(key, 0) + 1
              
              if not results:
                  report_md += "| ‚úÖ | None | No issues found! | 0 |\n"
              else:
                  for (level, rule, msg), count in sorted(results.items()):
                      report_md += f'| {level} | `{rule}` | {msg} | {count} |\n'
              
              return report_md, total_issues

          # --- Process Each Report ---
          all_reports = {
              "Trivy Vulnerability Scan": os.path.join(reports_dir, 'trivy-results', 'trivy-results.sarif'),
              "Ansible Lint": os.path.join(reports_dir, 'ansible-lint-results', 'ansible-lint-results.sarif')
              # Add other tools here as you generate more SARIF files
          }
          
          for tool_name, report_path in all_reports.items():
              with open(summary_file, 'a') as f:
                  f.write(f'## {tool_name} Report\n')
                  f.write('| Severity | Rule ID | Description | Total |\n')
                  f.write('|----------|---------|-------------|-------|\n')
              
              report_content, issue_count = parse_sarif(report_path)
              overall_total += issue_count
              
              with open(summary_file, 'a') as f:
                  f.write(report_content)
                  f.write('\n')

          # --- Final Summary ---
          with open(summary_file, 'a') as f:
            f.write('---\n')
            f.write(f'### Grand Total of All Issues Found: {overall_total}\n')

        shell: python